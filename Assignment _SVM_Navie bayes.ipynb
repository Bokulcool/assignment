{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdSe2BoxEzmz"
      },
      "outputs": [],
      "source": [
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Information Gain is a metric used in Decision Trees to measure how much a feature helps in reducing uncertainty or disorder (entropy) in the target variable when we split the data based on that feature. It quantifies the effectiveness of a feature in classifying the training data by calculating the reduction in entropy after the split.\n",
        "\n",
        "Mathematically, it is defined as:\n",
        "\n",
        "\"Information Gain\"(D,A)=H(D)-H(D∣A)\n",
        "\n",
        "where\n",
        "\n",
        "\tH(D) is the entropy of the dataset before the split,\n",
        "\n",
        "\tH(D∣A) is the conditional entropy of the dataset after splitting based on feature A.\n",
        "\n",
        "Entropy represents the impurity or variability in the dataset and is calculated as:\n",
        "\n",
        "H(D)=-∑_(i=1)^n p_i 〖log⁡〗_2 p_i\n",
        "\n",
        "\n",
        "where p_i is the probability of class i.\n",
        "\n",
        "Information Gain thus captures how much a dataset's impurity decreases by splitting on a particular feature. The feature that results in the highest Information Gain is chosen for the split at each node of the Decision Tree, as it leads to nodes that are more homogeneous (i.e., containing mostly one class).\n",
        "\n",
        "In summary, Information Gain is critical in Decision Trees because it guides the algorithm in selecting the best features for splitting, thereby creating efficient and accurate classification models.\n"
      ],
      "metadata": {
        "id": "2tqWEkxmE1tC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy?\n"
      ],
      "metadata": {
        "id": "rDPgGuP6FJUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gini Impurity and Entropy are both metrics used in Decision Trees to measure the impurity or disorder within a set of data points, guiding how the tree splits the data. They share a similar goal but differ in calculation, interpretation, and behavior:\n",
        "\n",
        "Gini Impurity\n",
        "\n",
        "\tMeasures the probability of a randomly chosen element being misclassified if it were labeled according to the distribution of classes in the node.\n",
        "\tCalculated as \"Gini\"=1-∑_(i=1)^C p_i^2, where p_i is the proportion of class i.\n",
        "\n",
        "\tRanges from 0 (pure node) to 0.5 (maximum impurity for binary classification).\n",
        "\n",
        "\tComputationally simpler and faster since it does not use logarithms.\n",
        "\tTends to isolate the most frequent class, sometimes favoring splits that focus on the dominant class.\n",
        "\n",
        "\tOften used in CART (Classification and Regression Tree) algorithms.\n",
        "\n",
        "Entropy\n",
        "\n",
        "\tMeasures the amount of uncertainty or disorder in the dataset.\n",
        "\tCalculated as \"Entropy\"=-∑_(i=1)^C p_i 〖log⁡〗_2 (p_i).\n",
        "\n",
        "\tRanges from 0 (pure node) to 1 (maximum impurity for binary classification).\n",
        "\tComputationally more intensive due to logarithms.\n",
        "\n",
        "\tTends to produce more balanced trees by focusing on reducing overall uncertainty.\n",
        "\n",
        "\tCommonly used in ID3 and C4.5 algorithms.\n",
        "\n",
        "Main Differences\n",
        "\n",
        "Feature             \tGini                        Impurity\tEntropy\n",
        "Interpretation\tProbability of misclassification\tAverage information needed to classify\n",
        "Range (binary class)\t 0 to 0.5                        \t0 to 1\n",
        "Computation\tSimpler,   no logarithms\t      Uses logarithmic operations\n",
        "Effect on splits\t     Prefers dominant class separation\t                  \n",
        "\n",
        "                                            Prefers balanced splits\n",
        "Used in algorithms    \tCART\tID3,                    C4.5\n",
        "\n"
      ],
      "metadata": {
        "id": "uBsza13HFNb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:What is Pre-Pruning in Decision"
      ],
      "metadata": {
        "id": "76phRkrjGMNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Pruning in Decision Trees, also known as early stopping, is a technique that stops the growth of the tree early during the training process to prevent it from becoming overly complex and overfitting the training data. Instead of allowing the tree to grow fully and then pruning back, pre-pruning sets constraints that halt the splitting of nodes when certain conditions are met.\n",
        "\n",
        "Common pre-pruning conditions include:\n",
        "\n",
        "Limiting the maximum depth of the tree.\n",
        "\n",
        "Setting a minimum number of samples required to split a node.\n",
        "\n",
        "Defining a minimum number of samples that must be present in a leaf node.\n",
        "\n",
        "Restricting the maximum number of features considered for splitting.\n",
        "\n",
        "By applying these constraints, pre-pruning results in a simpler, less complex tree that is less likely to overfit and generally quicker to train. However, it may risk underfitting if the growth is stopped too early, missing patterns in the data.\n",
        "\n",
        "In summary, pre-pruning prevents the growth of the tree beyond a point where further splitting does not significantly improve model performance, aiming for a balanced trade-off between complexity and accuracy"
      ],
      "metadata": {
        "id": "UTxRnim3GmgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n"
      ],
      "metadata": {
        "id": "DVBTFAIgGqXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train a Decision Tree Classifier using Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Print the feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n",
        "\n",
        "output\n",
        "\n",
        "Feature Importances:\n",
        "sepal length (cm): 0.0123\n",
        "sepal width (cm): 0.0000\n",
        "petal length (cm): 0.5584\n",
        "petal width (cm): 0.4293\n"
      ],
      "metadata": {
        "id": "a80xrW5GG07B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?"
      ],
      "metadata": {
        "id": "SGOZPgKbG_Bg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm primarily used for classification and regression tasks. It works by finding the optimal decision boundary, called a hyperplane, that best separates different classes of data points in the feature space. The key objective of SVM is to maximize the margin—the distance between the hyperplane and the nearest data points of each class, known as support vectors. A larger margin generally leads to better generalization on unseen data.\n",
        "\n",
        "SVM can handle both linearly separable and non-linearly separable data by utilizing kernel functions that transform data into higher-dimensional spaces where a linear separation becomes possible. This kernel trick enables SVMs to create complex decision boundaries without explicitly computing the coordinates in the higher-dimensional space.\n",
        "\n",
        "There are two main types of SVMs:\n",
        "\n",
        "Linear SVM which uses a linear hyperplane to separate data.\n",
        "\n",
        "Non-linear SVM which uses kernels like polynomial, Gaussian (RBF), or sigmoid to handle more complex data patterns.\n",
        "\n",
        "SVM also supports soft margins, allowing some misclassifications to improve the model’s ability to generalize when data is noisy or not perfectly separable.\n",
        "\n",
        "In summary, an SVM is a powerful classification algorithm that finds the optimal separating hyperplane by maximizing the margin between classes, supporting linear and nonlinear data separation through the kernel trick."
      ],
      "metadata": {
        "id": "wKbBI3ASGyK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?\n"
      ],
      "metadata": {
        "id": "Z-4S9PUgHEQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Kernel Trick in Support Vector Machines (SVM) is a technique that allows SVMs to efficiently handle non-linearly separable data by implicitly mapping the input features into a higher-dimensional space where the data can be linearly separated. Instead of explicitly computing the coordinates of data points in this high-dimensional feature space—which would be computationally expensive—the kernel trick uses kernel functions to directly compute the inner products of the transformed data points in this space.\n",
        "\n",
        "This means the SVM algorithm operates as if the data were transformed into a higher dimension to make it separable, but without ever performing the explicit transformation. Common kernel functions include:\n",
        "\n",
        "\tLinear kernel (no transformation, for already linearly separable data)\n",
        "\n",
        "\tPolynomial kernel (maps features into polynomial feature space)\n",
        "\n",
        "\tRadial Basis Function (RBF or Gaussian) kernel (handles complex decision boundaries)\n",
        "\n",
        "\tSigmoid kernel (similar to neural networks)\n",
        "\n",
        "Mathematically, the kernel trick replaces the inner product ϕ(x_i)⋅ϕ(x_j) in the higher-dimensional space with a kernel function K(x_i,x_j) computed in the original space.\n",
        "\n",
        "This approach vastly expands the power of SVMs to classify data that are not linearly separable in the original feature space while keeping computation efficient. It enables SVMs to learn very complex boundaries without losing the advantages of the algorithm's theoretical foundation.\n",
        "\n",
        "In summary, the Kernel Trick allows SVMs to solve non-linear classification problems by implicitly mapping features to higher dimensions through kernel functions, avoiding heavy computation and enabling flexible, powerful classification\n"
      ],
      "metadata": {
        "id": "VtAsWWDVHG_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies."
      ],
      "metadata": {
        "id": "JbUa2XwKHZ5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"SVM with Linear Kernel Accuracy:\", accuracy_linear)\n",
        "print(\"SVM with RBF Kernel Accuracy:\", accuracy_rbf)\n",
        "\n",
        "Output\n",
        "\n",
        "SVM with Linear Kernel Accuracy: 0.9815\n",
        "SVM with RBF Kernel Accuracy: 0.9074\n"
      ],
      "metadata": {
        "id": "Tv_PZPzvHfGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n"
      ],
      "metadata": {
        "id": "OuzpfyacHpAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Naïve Bayes classifier is a supervised machine learning algorithm that classifies data points based on probabilities derived from Bayes’ Theorem. It predicts the category of a data point by calculating the probability that it belongs to each class and then choosing the class with the highest probability. The classification is done by assuming that all features (predictors) are conditionally independent of each other given the class label.\n",
        "\n",
        "It is called \"Naïve\" because this assumption of feature independence is highly simplistic and often unrealistic in real-world data where features can be correlated. Despite this \"naïve\" assumption, the classifier performs surprisingly well in many applications like spam filtering, text classification, and sentiment analysis due to its simplicity, efficiency, and relatively good accuracy.\n",
        "\n",
        "In essence, the Naïve Bayes classifier combines:\n",
        "\n",
        "Bayes’ theorem to compute the posterior probability of each class given the features.\n",
        "\n",
        "The naive independence assumption, treating each feature’s contribution to the class probability as independent.\n",
        "\n",
        "This simplicity allows it to train quickly and requires less data to estimate parameters compared to more complex classifiers, but it may produce less accurate probability estimates when feature dependencies are strong.\n",
        "\n",
        "To summarize, Naïve Bayes is a probabilistic classifier named for its \"naïve\" assumption of feature independence, which enables efficient and effective classification despite the unrealistic simplification"
      ],
      "metadata": {
        "id": "W9v7UMg3Hs-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n"
      ],
      "metadata": {
        "id": "S2bNl3A0H2Fp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes are three variants of the Naïve Bayes classifier, each suited for different types of data based on the underlying assumptions about feature distributions:\n",
        "\n",
        "Gaussian Naïve Bayes assumes that continuous features follow a Gaussian (normal) distribution. It is commonly used when the data have continuous values like height, weight, or any measurements that can be modeled by bell-shaped curves. For example, it can be used for classifying iris flower species based on sepal and petal dimensions.\n",
        "\n",
        "Multinomial Naïve Bayes is designed for discrete count data such as word frequencies in text classification. It models the features as multinomially distributed, which means it considers how many times each feature (e.g., a word) occurs in a document. This makes it suitable for tasks like spam detection or document categorization based on term counts.\n",
        "\n",
        "Bernoulli Naïve Bayes works with binary/Boolean features indicating the presence or absence of an attribute (e.g., whether a word occurs or not in a document). It is useful when data are represented as binary vectors, for instance in text classification problems where features indicate whether a word appears in a document regardless of its frequency."
      ],
      "metadata": {
        "id": "Cr__4famIAPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy."
      ],
      "metadata": {
        "id": "CTldsHvVIHgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Gaussian Naive Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"Gaussian Naive Bayes Classifier Accuracy:\", accuracy)\n",
        "\n",
        "Output\n",
        "Gaussian Naive Bayes Classifier Accuracy: 0.9532\n"
      ],
      "metadata": {
        "id": "QUSWlDjrIQ3R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}