{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjDnngVTft3h"
      },
      "outputs": [],
      "source": [
        "1 What is unsupervised learning in the context of machine learn?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsupervised learning in machine learning is a type of learning where the algorithm is trained on data that does not have labeled responses or predefined categories. Unlike supervised learning, where input-output pairs guide the model training, unsupervised learning algorithms analyze only the input data and aim to discover hidden patterns, structures, or relationships within the data independently. This process involves grouping similar data points together (clustering), reducing data dimensionality, or detecting anomalies without any human-provided labels. The ultimate goal is to uncover meaningful insights or organize data based on inherent similarities or structures without prior knowledge of the data's true categories or outcomes"
      ],
      "metadata": {
        "id": "ulMhtgjCgMB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.How does K-Means clustering algorithm work?"
      ],
      "metadata": {
        "id": "ODYLqBvKgeuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The K-Means clustering algorithm works as follows:\n",
        "\n",
        "\tSpecify the number of clusters, k, to create.\n",
        "\n",
        "\tRandomly initialize k cluster centroids (means).\n",
        "\n",
        "\tAssign each data point to the nearest centroid based on distance (usually Euclidean).\n",
        "\n",
        "\tRecalculate the centroids as the mean of all points assigned to each cluster.\n",
        "\n",
        "\tRepeat steps 3 and 4 until the cluster assignments no longer change or reach a maximum number of iterations.\n",
        "  \n",
        "The goal is to minimize the within-cluster sum of squares (WCSS), which quantifies the variance within each cluster. K-Means iteratively refines cluster centers and memberships until convergence, effectively partitioning the data into groups of similar points.\n"
      ],
      "metadata": {
        "id": "Hp_gsKPzgoG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Explain the concept of a dendrogram in hierarchical clustering?"
      ],
      "metadata": {
        "id": "bkUoqhWFg6AH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dendrogram in hierarchical clustering is a tree-like diagram that visualizes how individual data points or clusters are progressively merged or split. It represents the hierarchy of clusters formed during the clustering process.\n",
        "\n",
        "At the bottom of the dendrogram, every data point starts as its own cluster. As you move up, clusters that are most similar (or closest) merge together, forming larger clusters. The height at which two clusters merge indicates their dissimilarity or distance—the lower the height, the more similar the clusters are. This visual representation helps to understand the structure and relationships in the data, and it allows selecting the number of clusters by \"cutting\" the dendrogram at a chosen height.\n",
        "\n",
        "Essentially, a dendrogram provides an intuitive way to see how clusters nest within each other and to decide on an appropriate clustering by analyzing the distances between merges"
      ],
      "metadata": {
        "id": "2DQ6H7q_hEq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q. What is the main difference between K-Means and Hierarchical Clustering?"
      ],
      "metadata": {
        "id": "ZAh67_-KhLcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main difference between K-Means and Hierarchical Clustering lies in their approach to grouping data:\n",
        "\n",
        "K-Means is a partitional clustering algorithm that requires the number of clusters,\n",
        "k\n",
        "k, to be specified beforehand. It iteratively assigns data points to the nearest cluster centroid and updates centroids until convergence, producing flat, non-overlapping clusters. It assumes clusters are spherical and similarly sized, and is computationally efficient for large datasets.\n",
        "\n",
        "Hierarchical Clustering builds a tree-like structure called a dendrogram without needing to predefine the number of clusters. It either merges points/clusters progressively (agglomerative) or splits a single cluster recursively (divisive), allowing exploration of cluster relationships at different levels. It can capture clusters of various shapes and sizes but is more computationally intensive, typically suited for smaller datasets.\n",
        "\n",
        "In summary, K-Means partitions data into a fixed number of flat clusters efficiently, while Hierarchical Clustering creates a multi-level hierarchy of clusters offering flexibility in cluster selection and deeper insight into data structure"
      ],
      "metadata": {
        "id": "mu_TzAgdhRU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What are the advantages of DBSCAN over K-Means?"
      ],
      "metadata": {
        "id": "oVPsAyXBhbm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN has several advantages over K-Means clustering:\n",
        "\n",
        "Ability to find clusters of arbitrary shape: DBSCAN identifies clusters based on density and can detect non-spherical, irregularly shaped clusters, while K-Means assumes spherical clusters and tends to perform poorly on complex shapes.\n",
        "\n",
        "No need to specify the number of clusters: DBSCAN automatically determines the number of clusters based on data density, whereas K-Means requires specifying the number of clusters\n",
        "k k upfront.\n",
        "\n",
        "Robustness to noise and outliers: DBSCAN explicitly labels low-density points as noise and excludes them from clusters, improving cluster quality. K-Means assigns every point to a cluster, which can distort cluster boundaries if outliers exist.\n",
        "\n",
        "Handles clusters with varying densities: DBSCAN can recognize clusters with different densities, while K-Means assumes clusters have similar densities and sizes.\n",
        "\n",
        "However, DBSCAN can be sensitive to its parameters (epsilon radius and minimum points) and may struggle with very high-dimensional datasets. But overall, DBSCAN is preferred in scenarios where clusters have complex shapes, the number of clusters is unknown, and noise/outliers are present"
      ],
      "metadata": {
        "id": "IjaVGl3Ehjhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. When would you use Silhouette Score in clustering?"
      ],
      "metadata": {
        "id": "_hZxxYighw5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Silhouette Score is used in clustering to evaluate the quality of clusters formed by a clustering algorithm. It measures how well each data point fits within its assigned cluster compared to other clusters by considering two key factors:\n",
        "\n",
        "Cohesion: How close the point is to other points in the same cluster (intra-cluster distance).\n",
        "\n",
        "Separation: How far the point is from points in the nearest neighboring cluster (nearest-cluster distance).\n",
        "\n",
        "The Silhouette Score ranges from -1 to +1:\n",
        "\n",
        "A score close to +1 indicates that points are well matched to their own cluster and distinctly separated from others.\n",
        "\n",
        "A score near 0 suggests points lie between clusters or clusters overlap.\n",
        "\n",
        "A score close to -1 means points may be misclassified.\n",
        "\n",
        "It is commonly used to:\n",
        "\n",
        "Assess clustering effectiveness when no ground truth labels exist,\n",
        "\n",
        "Help determine the optimal number of clusters by comparing average silhouette scores across different cluster counts,\n",
        "\n",
        "Visually analyze cluster cohesion and separation.\n",
        "\n",
        "Thus, Silhouette Score is a valuable internal validation metric for clustering evaluation, especially when deciding how well-separated and well-formed the clusters are"
      ],
      "metadata": {
        "id": "JyZbutw5iDm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.What are the limitations of Hierarchical Clustering?"
      ],
      "metadata": {
        "id": "1zlv49t-iKlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hierarchical clustering has several limitations:\n",
        "\n",
        "\tComputationally Expensive: It has a time complexity of roughly O(n^3), making it inefficient and slow for large datasets.\n",
        "\n",
        "\tSensitivity to Noise and Outliers: Outliers can significantly distort the clustering results and the dendrogram structure.\n",
        "\n",
        "\tIrreversibility: Once clusters are merged or split in the hierarchical process, the decision cannot be undone, which may lead to suboptimal clustering.\n",
        "\n",
        "\tDifficulty in Choosing Parameters: Selecting the appropriate linkage criterion and distance metric can greatly influence results, and there is no definitive method for choosing them.\n",
        "\n",
        "\tInterpretability Challenges: The dendrogram can be complex and hard to interpret, especially for large or high-dimensional data.\n",
        "\n",
        "\tSensitivity to Data Order: The clustering outcome may depend on the order in which data points are processed.\n",
        "\n",
        "\tLimited Scalability: Due to its computational demands, it is not suitable for very large datasets.\n",
        "\n",
        "\tChallenges with Mixed or Categorical Data: Hierarchical clustering works best with numeric data and struggles with categorical variables without proper encoding.\n",
        "\n",
        "These limitations make hierarchical clustering more appropriate for smaller datasets or cases where understanding hierarchical relationships is important\n"
      ],
      "metadata": {
        "id": "ZXmkWRv4iQow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Why is feature scaling important in clustering algorithms like K-Means?"
      ],
      "metadata": {
        "id": "XUL0njnbinXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is important in clustering algorithms like K-Means because these algorithms rely on distance calculations (commonly Euclidean distance) to assign points to clusters. If the features have different scales or units, those with larger ranges can dominate the distance calculations, causing the clustering results to be biased toward those features.\n",
        "\n",
        "Scaling features to a uniform range ensures that all features contribute equally to the distance metric, preventing any feature from disproportionately influencing cluster assignments. This leads to more accurate, meaningful cluster structures and often improves the algorithm's convergence speed.\n",
        "\n",
        "Hence, feature scaling is crucial especially when the dataset contains features measured in different units or ranges to achieve reliable and consistent clustering outcomes"
      ],
      "metadata": {
        "id": "zXwZq5EQiyMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. How does DBSCAN identify noise points?"
      ],
      "metadata": {
        "id": "vfUWpz9ui6De"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN identifies noise points as those which do not belong to any cluster based on density criteria. Specifically:\n",
        "\n",
        "For each point, DBSCAN counts the number of points within a radius ε (epsilon), called the neighborhood.\n",
        "\n",
        "If the number of points in the neighborhood is less than a minimum threshold MinPts, the point is not dense enough to be a core point.\n",
        "\n",
        "Points that are neither core points (with sufficient neighbors) nor reachable from any core point are labeled as noise points.\n",
        "\n",
        "Noise points are isolated from dense regions and are not assigned to any cluster.\n",
        "\n",
        "In summary, noise points are defined as those that do not have enough neighboring points within the ε radius and cannot be density-connected to any core point cluster, effectively identifying outliers or sparse areas in the data"
      ],
      "metadata": {
        "id": "uHXY4E_cjAKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Define inertia in the context of K-Means?"
      ],
      "metadata": {
        "id": "AOSPfpUCjJZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of K-Means clustering, inertia is defined as the sum of the squared distances between each data point and the centroid of the cluster to which it is assigned. It measures how internally coherent the clusters are; lower inertia indicates that the points are closer to their respective cluster centers, implying more compact clusters.\n",
        "Mathematically, inertia is the total within-cluster sum of squares (WCSS), calculated as:\n",
        "\n",
        "\"Inertia\"=∑_(i=1)^n \"distance\"(x_i,c_j^* )^2\n",
        "\n",
        "where x_i is a data point, and c_j^* is the centroid of the cluster nearest to x_i.\n",
        "\n",
        "Inertia is commonly used in the Elbow Method to identify the optimal number of clusters k by plotting inertia against different k values and looking for the \"elbow point\" where the decrease in inertia starts to level off, indicating diminishing returns in cluster compactness improvement.\n"
      ],
      "metadata": {
        "id": "dhQiNGKtjR3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.What is the elbow method in K-Means clustering?"
      ],
      "metadata": {
        "id": "baVQCa1Qje-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The elbow method in K-Means clustering is a heuristic technique used to determine the optimal number of clusters (K) in the data. It involves the following steps:\n",
        "1.\tRun K-Means clustering on the dataset for a range of K values (e.g., 1 to 10).\n",
        "\n",
        "2.\tFor each K, calculate the within-cluster sum of squares (WCSS) or inertia, which measures how compact the clusters are.\n",
        "\n",
        "3.\tPlot the WCSS values against the number of clusters K.\n",
        "\n",
        "4.\tLook for the \"elbow point\" on the curve where adding more clusters results in only a small decrease in WCSS, indicating diminishing returns.\n",
        "\n",
        "5.\tThe K value at this elbow point is considered optimal because it balances cluster compactness with model complexity.\n",
        "\n",
        "The elbow method helps select a K that avoids both underfitting (too few clusters) and overfitting (too many clusters), providing a practical way to choose the cluster count based on data structure\n"
      ],
      "metadata": {
        "id": "6KY-7W0yjp9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.Describe the concept of \"density\" in DBSCAN?"
      ],
      "metadata": {
        "id": "sucirblRj36b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The concept of \"density\" in DBSCAN (Density-Based Spatial Clustering of Applications with Noise) refers to how closely packed data points are in a region of the data space. Specifically, density is determined by two parameters:\n",
        "\n",
        "Epsilon (ε) - The radius around a data point defining its neighborhood.\n",
        "\n",
        "MinPts - The minimum number of points required within this ε-radius neighborhood to qualify the point as part of a dense region.\n",
        "\n",
        "A point is considered a core point if it has at least MinPts neighbors within the ε distance, indicating it lies in a dense area. Points that are reachable from core points but do not themselves meet the density criteria are border points, while points that are neither core nor reachable border points are considered noise or outliers.\n",
        "\n",
        "Thus, DBSCAN defines clusters as areas of high density separated by regions of low density, enabling it to discover clusters of arbitrary shape and identify noise effectively based on the spatial density of points."
      ],
      "metadata": {
        "id": "LUjubPZuj8uj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.Can hierarchical clustering be used on categorical data?"
      ],
      "metadata": {
        "id": "Y0r1YC6fkDtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, hierarchical clustering can be used on categorical data, but it requires specialized handling since traditional hierarchical clustering relies on distance measures like Euclidean distance, which are not suitable for categorical variables.\n",
        "\n",
        "For categorical data, alternative similarity or distance measures such as Hamming distance, Jaccard distance, or Gower’s distance are employed. These metrics effectively capture dissimilarities based on category mismatches. The hierarchical clustering process then proceeds similarly, either merging or splitting clusters based on these distances.\n",
        "\n",
        "Additionally, categorical data may be preprocessed through appropriate encoding methods or use of dissimilarity matrices. The resulting dendrogram visualizes the nested cluster structure and helps in selecting the number of clusters by cutting the tree at different heights.\n",
        "\n",
        "This approach is commonly applied in domains like customer segmentation, survey analysis, or document grouping where data is categorical"
      ],
      "metadata": {
        "id": "PsfInWELkJQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What does a negative Silhouette Score indicate?"
      ],
      "metadata": {
        "id": "_FSjhjtqkQCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A negative Silhouette Score indicates that a data point may be assigned to the wrong cluster. Specifically, it means that the point is, on average, closer to points in a neighboring cluster than to points within its own assigned cluster. This suggests poor clustering quality for that point and possibly for the clustering solution overall.\n",
        "\n",
        "Intuitively, negative values imply misclassification or that the cluster boundaries are not well defined for those points. If many points have negative silhouette values, it may indicate too many or too few clusters or that the data naturally doesn't cluster well at the chosen granularity.\n",
        "\n",
        "Thus, a negative Silhouette Score is a strong signal to reconsider the clustering configuration or the number of clusters."
      ],
      "metadata": {
        "id": "nxF8DZsdkVWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.Explain the term \"linkage criteria\" in hierarchical clustering?"
      ],
      "metadata": {
        "id": "_Xm0T7ZlkbWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In hierarchical clustering, the term \"linkage criteria\" refers to the rule or method used to measure the distance or dissimilarity between clusters when deciding which clusters to merge at each step. Linkage criteria determine how the distance between two clusters is computed based on the pairwise distances between data points in those clusters.\n",
        "\n",
        "Common types of linkage criteria include:\n",
        "\n",
        "Single Linkage: Distance between the closest points of two clusters (minimum distance). It tends to produce elongated, chain-like clusters.\n",
        "\n",
        "Complete Linkage: Distance between the farthest points of two clusters (maximum distance). It favors compact, spherical clusters.\n",
        "\n",
        "Average Linkage: Average distance between all pairs of points in the two clusters, providing a balance between single and complete linkage.\n",
        "\n",
        "Ward's Linkage: Minimizes the increase in total within-cluster variance after merging, producing clusters with minimum variance.\n",
        "\n",
        "Choosing the appropriate linkage criterion impacts the shape and size of clusters and the resulting dendrogram structure in hierarchical clustering"
      ],
      "metadata": {
        "id": "b7WLSr52kgki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?"
      ],
      "metadata": {
        "id": "lttdM5AukmZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means clustering can perform poorly on data with varying cluster sizes or densities because it assumes clusters are spherical and have roughly equal sizes and densities. This assumption causes several issues:\n",
        "\n",
        "Unequal Cluster Sizes: K-Means assigns points to the nearest centroid, which works best when clusters are similar in size. Larger or smaller clusters may be split or merged incorrectly.\n",
        "\n",
        "Varying Densities: K-Means uses distance to centroids without considering density, so clusters with different point densities may not be properly separated.\n",
        "\n",
        "Cluster Shape Assumption: K-Means favors spherical (globular) clusters, thus struggling on elongated or irregular-shaped clusters.\n",
        "\n",
        "Sensitivity to Outliers: Outliers can shift the cluster centroid, distorting cluster assignments especially when cluster densities vary.\n",
        "\n",
        "Due to these limitations, K-Means may split large clusters inaccurately and merge smaller or less dense ones, leading to suboptimal clustering results on such data distributions. More flexible algorithms like DBSCAN or Gaussian Mixture Models are preferred for data with varying cluster sizes and densities."
      ],
      "metadata": {
        "id": "NQjMTVe3ksLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are the core parameters in DBSCAN, and how do they influence cluster?"
      ],
      "metadata": {
        "id": "hgtL1bjBkzTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The core parameters in DBSCAN are:\n",
        "\n",
        "Epsilon (ε): This defines the radius of the neighborhood around a data point. Two points are considered neighbors if the distance between them is less than or equal to ε. The choice of ε determines the scale at which the algorithm searches for dense regions. If ε is too small, many points will be classified as noise; if too large, clusters may merge incorrectly.\n",
        "\n",
        "MinPts (Minimum Points): This specifies the minimum number of points required in an ε-radius neighborhood for a point to be considered a core point, indicating a dense region. The value of MinPts influences cluster density definition; smaller MinPts may detect smaller clusters but be sensitive to noise, while larger MinPts require denser clusters.\n",
        "\n",
        "Together, these parameters control how DBSCAN defines clusters based on density and how it distinguishes core points, border points, and noise, thereby influencing the shape, size, and number of clusters detected in the data"
      ],
      "metadata": {
        "id": "jtA4F4Bbk5jR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.How does K-Means++ improve upon standard K-Means initialization?"
      ],
      "metadata": {
        "id": "2pRfxuP-lBWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means++ improves upon standard K-Means initialization by choosing initial cluster centers more strategically rather than randomly.\n",
        "\n",
        "In K-Means++, the first centroid is chosen randomly from the data points, but each subsequent centroid is selected with a probability proportional to the square of the distance from the point to the nearest already chosen centroid. This approach ensures the initial centroids are spread out across the data space.\n",
        "\n",
        "By spreading out the initial cluster centers, K-Means++ often leads to faster convergence and better clustering results, avoiding poor solutions where centroids cluster too close together or some clusters are left empty, which can happen with random initialization.\n",
        "\n",
        "Though the initialization step is computationally more involved than standard K-Means, the overall run-time is typically reduced due to fewer iterations needed for convergence and more stable clustering."
      ],
      "metadata": {
        "id": "rN03gBWqlL7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.What is agglomerative clustering?"
      ],
      "metadata": {
        "id": "seRylOAhlNFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agglomerative clustering is a type of hierarchical clustering that follows a bottom-up approach. It begins by treating each data point as its own individual cluster. Then, iteratively, it merges the two closest or most similar clusters step-by-step based on a chosen distance metric and linkage criteria. This merging process continues until all points are combined into a single cluster or until a stopping criterion is reached.\n",
        "\n",
        "The result is a hierarchy of clusters represented by a dendrogram, which shows how clusters are nested and merged at various levels of similarity.\n",
        "\n",
        "Agglomerative clustering is useful for discovering the structure in data without pre-specifying the number of clusters, and it is good at identifying small and nested clusters."
      ],
      "metadata": {
        "id": "ymKGdr40lUp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What makes Silhouette Score a better metric than just inertia for model evaluation?"
      ],
      "metadata": {
        "id": "7pNHL5lLldRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Silhouette Score is generally considered a better metric than inertia for clustering model evaluation because it captures both cohesion (how close points are within the same cluster) and separation (how well clusters are separated from each other), whereas inertia only measures cluster compactness.\n",
        "\n",
        "Key reasons why Silhouette Score is better than inertia:\n",
        "\n",
        "Considers Cluster Separation: Silhouette Score evaluates how far data points are from neighboring clusters in addition to their closeness within clusters. Inertia ignores separation and focuses solely on minimizing intra-cluster variance.\n",
        "\n",
        "Scale-Free Interpretation: Silhouette values range from -1 to +1, providing an interpretable scale where values near +1 indicate well-defined clusters, values near 0 indicate overlapping clusters, and negative values suggest misclassification. Inertia is an unbounded quantity sensitive to data scale.\n",
        "\n",
        "Prevents Overfitting on Cluster Count: Inertia monotonically decreases with more clusters, encouraging overfitting with too many clusters. Silhouette Score often identifies a peak value indicating a balanced number of clusters.\n",
        "\n",
        "Reflects Overall Cluster Quality: Silhouette Score reflects cluster consistency and separation for individual points and averages them for global cluster structure insights. Inertia aggregates within-cluster distances only.\n",
        "\n",
        "Thus, Silhouette Score is more robust for evaluating the goodness of clustering and choosing the optimal number of clusters because it balances the internal cohesion and external separation of clusters, unlike inertia which focuses narrowly on compactness."
      ],
      "metadata": {
        "id": "1wWScwwclh-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                     Practicle"
      ],
      "metadata": {
        "id": "Q59IG_cWlrbP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a\n",
        "scatter plot?"
      ],
      "metadata": {
        "id": "q3MZ8RL-luT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# 1. Generate synthetic data with 4 centers\n",
        "X, y_true = make_blobs(n_samples=500, centers=4, cluster_std=0.8, random_state=42)\n",
        "\n",
        "# 2. Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "# 3. Visualize the clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', s=50, alpha=0.7)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            c='red', s=200, marker='X', label='Centroids')\n",
        "\n",
        "plt.title('K-Means Clustering (4 Clusters)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "U4FgHdQMmwiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels?"
      ],
      "metadata": {
        "id": "KHlJmYdym47p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Perform Agglomerative Clustering with 3 clusters\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=3)\n",
        "labels = agg_clustering.fit_predict(X)\n",
        "\n",
        "# Display the first 10 predicted labels\n",
        "print(labels[:10])\n"
      ],
      "metadata": {
        "id": "M4C_MNnnnR7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23 Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot?"
      ],
      "metadata": {
        "id": "eVqXBrO6nWHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Generate synthetic data (two interleaving half circles)\n",
        "X, y_true = make_moons(n_samples=300, noise=0.07, random_state=42)\n",
        "\n",
        "# 2. Standardize data (DBSCAN is distance-based, scaling is important)\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# 3. Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.3, min_samples=5)  # eps = neighborhood radius, min_samples = minimum points per cluster\n",
        "y_dbscan = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# 4. Identify outliers (DBSCAN labels outliers as -1)\n",
        "outliers = (y_dbscan == -1)\n",
        "\n",
        "# 5. Visualize clusters and outliers\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_scaled[~outliers, 0], X_scaled[~outliers, 1],\n",
        "            c=y_dbscan[~outliers], cmap='plasma', s=50, label='Clusters')\n",
        "plt.scatter(X_scaled[outliers, 0], X_scaled[outliers, 1],\n",
        "            c='black', s=60, marker='x', label='Outliers')\n",
        "\n",
        "plt.title('DBSCAN Clustering on make_moons Dataset')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GtuZ8rkOnePJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster?"
      ],
      "metadata": {
        "id": "XmMzWCNwnkPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply K-Means clustering with 3 clusters\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Print the size of each cluster\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "cluster_sizes = dict(zip(unique, counts))\n",
        "print(\"Cluster sizes:\", cluster_sizes)\n"
      ],
      "metadata": {
        "id": "FsR2wcoZnrxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result?"
      ],
      "metadata": {
        "id": "U8clvNWen74M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Generate synthetic concentric circles data\n",
        "X, y = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)\n",
        "\n",
        "# Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.1, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Plot the clustering result\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
        "plt.title('DBSCAN Clustering on Synthetic Circles Data')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "y328nND_oCg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster\n",
        "centroids?"
      ],
      "metadata": {
        "id": "7rgCq4z1oRCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "\n",
        "# Scale features using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply K-Means clustering with 2 clusters\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "# Output cluster centroids\n",
        "print(\"Cluster Centroids:\\n\", kmeans.cluster_centers_)\n"
      ],
      "metadata": {
        "id": "xnmNO8zFoURq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with\n",
        "DBSCAN?"
      ],
      "metadata": {
        "id": "XzgWUTkeobsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Generate synthetic data with varying cluster standard deviations\n",
        "X, y = make_blobs(\n",
        "    n_samples=500,\n",
        "    centers=[[0, 0], [5, 5], [10, 0]],\n",
        "    cluster_std=[0.5, 1.5, 0.3],  # Different std dev for each cluster\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.8, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Plot the clustering results\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='plasma', s=50)\n",
        "plt.title('DBSCAN Clustering on Synthetic Blobs with Varying Std Dev')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "I6N1zipVojon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means?"
      ],
      "metadata": {
        "id": "s9ruBv_botEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Reduce dimensionality to 2D using PCA\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "labels = kmeans.fit_predict(X_reduced)\n",
        "\n",
        "# Plot the clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=labels, cmap='tab10', s=50)\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.title('K-Means Clusters on PCA-Reduced Digits Data')\n",
        "plt.colorbar(scatter, label='Cluster Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MRMmEO67oy6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30.Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage?"
      ],
      "metadata": {
        "id": "8YcqvObTo4Qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Standardize the data (important for distance-based methods)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Perform Hierarchical Clustering with average linkage\n",
        "Z = linkage(X_scaled, method='average')\n",
        "\n",
        "# 4. Plot the dendrogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "dendrogram(Z, truncate_mode=None, p=150, leaf_rotation=90, leaf_font_size=10)\n",
        "plt.title('Hierarchical Clustering Dendrogram (Average Linkage) - Iris Dataset')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Distance')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iF6_IkHEsNMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with\n",
        "decision boundaries?"
      ],
      "metadata": {
        "id": "y-2Eo0Sps-Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Generate synthetic data with overlapping clusters\n",
        "X, y_true = make_blobs(n_samples=500, centers=3, cluster_std=2.0, random_state=42)\n",
        "\n",
        "# Standardize features for better clustering\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 2. Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# 3. Create a mesh grid for decision boundaries\n",
        "x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1\n",
        "y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n",
        "                     np.linspace(y_min, y_max, 500))\n",
        "\n",
        "# Predict cluster labels for each point in the grid\n",
        "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# 4. Visualize clusters with decision boundaries\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_kmeans, cmap='viridis', s=50, edgecolor='k')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            c='red', s=200, marker='X', label='Centroids')\n",
        "\n",
        "plt.title('K-Means Clustering with Decision Boundaries (Overlapping Clusters)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t2zjQN7io-va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32.Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results?"
      ],
      "metadata": {
        "id": "SJrbzaR6tON7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Load Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Reduce dimensionality to 2D using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_embedded = tsne.fit_transform(X)\n",
        "\n",
        "# Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=3.0, min_samples=5)\n",
        "labels = dbscan.fit_predict(X_embedded)\n",
        "\n",
        "# Plot clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=labels, cmap='tab10', s=50)\n",
        "plt.title(\"DBSCAN Clustering on t-SNE Reduced Digits Data\")\n",
        "plt.xlabel(\"t-SNE Dim 1\")\n",
        "plt.ylabel(\"t-SNE Dim 2\")\n",
        "plt.colorbar(label='Cluster Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gjHdcMlLtUZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot\n",
        "the result?"
      ],
      "metadata": {
        "id": "GQl6V8dita7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Generate synthetic data\n",
        "X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=1.5, random_state=42)\n",
        "\n",
        "# 2. Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Apply Agglomerative Clustering with complete linkage\n",
        "agg_clust = AgglomerativeClustering(n_clusters=3, linkage='complete')\n",
        "y_pred = agg_clust.fit_predict(X_scaled)\n",
        "\n",
        "# 4. Visualize the clustering result\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_pred, cmap='viridis', s=50, edgecolor='k')\n",
        "plt.title('Agglomerative Clustering (Complete Linkage)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wxoRxl5rtgpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34.Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a\n",
        "line plot?"
      ],
      "metadata": {
        "id": "yvMCdy4ptmjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "\n",
        "# 2. Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Compute K-Means inertia values for K = 2 to 6\n",
        "inertias = []\n",
        "K_values = range(2, 7)\n",
        "\n",
        "for k in K_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "# 4. Plot inertia vs K\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(K_values, inertias, marker='o', linestyle='-', color='blue')\n",
        "plt.title('K-Means Inertia for Different K Values (Breast Cancer Dataset)')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4C5kjPeYtsOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35.Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with single linkage?"
      ],
      "metadata": {
        "id": "Fnf-dRbctxhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Generate synthetic concentric circles data\n",
        "X, y_true = make_circles(n_samples=400, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "# 2. Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Apply Agglomerative Clustering with single linkage\n",
        "agg_cluster = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "y_pred = agg_cluster.fit_predict(X_scaled)\n",
        "\n",
        "# 4. Visualize clustering results\n",
        "plt.figure(figsize=(7, 6))\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_pred, cmap='plasma', s=50, edgecolor='k')\n",
        "plt.title(\"Agglomerative Clustering (Single Linkage) on Concentric Circles\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TSUqyb5jt6dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters excluding noise?"
      ],
      "metadata": {
        "id": "vKrGenHtuAwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# 2. Standardize the data (important for DBSCAN)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=1.5, min_samples=5)\n",
        "y_db = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# 4. Count the number of clusters (excluding noise labeled as -1)\n",
        "n_clusters = len(set(y_db)) - (1 if -1 in y_db else 0)\n",
        "n_noise = list(y_db).count(-1)\n",
        "\n",
        "print(f\"Number of clusters (excluding noise): {n_clusters}\")\n",
        "print(f\"Number of noise points: {n_noise}\")\n"
      ],
      "metadata": {
        "id": "evUxZn0VuPV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37 Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the data points?"
      ],
      "metadata": {
        "id": "IOfFnvPiuWBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# 1. Generate synthetic data\n",
        "X, y_true = make_blobs(n_samples=400, centers=3, cluster_std=1.0, random_state=42)\n",
        "\n",
        "# 2. Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "# 3. Get cluster centers\n",
        "centers = kmeans.cluster_centers_\n",
        "\n",
        "# 4. Plot the clustered data and cluster centers\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis', edgecolor='k', alpha=0.7)\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, marker='X', label='Cluster Centers')\n",
        "plt.title('K-Means Clustering with Cluster Centers')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pqPM3U83uZxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38 Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise?"
      ],
      "metadata": {
        "id": "Eif4uGP9ufAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# 2. Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.8, min_samples=5)\n",
        "y_db = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# 4. Count how many samples are identified as noise\n",
        "n_noise = list(y_db).count(-1)\n",
        "n_clusters = len(set(y_db)) - (1 if -1 in y_db else 0)\n",
        "\n",
        "print(f\"Number of clusters (excluding noise): {n_clusters}\")\n",
        "print(f\"Number of noise samples: {n_noise}\")\n"
      ],
      "metadata": {
        "id": "VYqz0OqpumQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39 Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result?"
      ],
      "metadata": {
        "id": "GLChbSZAu0v0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Generate synthetic non-linear data (two interleaving half-moons)\n",
        "X, y_true = make_moons(n_samples=400, noise=0.1, random_state=42)\n",
        "\n",
        "# 2. Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# 4. Visualize the clustering result\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_kmeans, cmap='viridis', s=50, edgecolor='k')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            c='red', s=200, marker='X', label='Cluster Centers')\n",
        "plt.title(\"K-Means Clustering on Non-linearly Separable Data (make_moons)\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O_CucivBu79k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40.Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D\n",
        "scatter plot.?"
      ],
      "metadata": {
        "id": "WoB5kSFivEL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# 1. Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y_true = digits.target\n",
        "\n",
        "# 2. Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Apply PCA to reduce dimensions to 3\n",
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# 4. Apply K-Means clustering (choose 10 clusters for digits 0–9)\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# 5. 3D Visualization\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],\n",
        "                     c=y_kmeans, cmap='tab10', s=40, alpha=0.8, edgecolor='k')\n",
        "\n",
        "ax.set_title('K-Means Clustering on Digits Dataset (PCA 3D Visualization)', fontsize=12)\n",
        "ax.set_xlabel('Principal Component 1')\n",
        "ax.set_ylabel('Principal Component 2')\n",
        "ax.set_zlabel('Principal Component 3')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6spcLkcwvJ7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41.Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the clustering?"
      ],
      "metadata": {
        "id": "ILG0vXYgvTbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Generate synthetic data with 5 centers\n",
        "X, y_true = make_blobs(n_samples=500, centers=5, cluster_std=1.0, random_state=42)\n",
        "\n",
        "# 2. Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "# 3. Evaluate clustering using Silhouette Score\n",
        "score = silhouette_score(X, y_kmeans)\n",
        "print(f\"Silhouette Score for K-Means with 5 clusters: {score:.3f}\")\n",
        "\n",
        "# 4. Visualize the clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', s=50, edgecolor='k', alpha=0.7)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            c='red', s=200, marker='X', label='Cluster Centers')\n",
        "plt.title('K-Means Clustering (5 Centers) with Silhouette Evaluation')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KOM0t8iUvdDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42.Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering. Visualize in 2D?"
      ],
      "metadata": {
        "id": "SIVGJJkdvlt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Apply PCA to reduce dimensionality to 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# 4. Apply Agglomerative Clustering (e.g., 2 clusters for benign/malignant)\n",
        "agg = AgglomerativeClustering(n_clusters=2, linkage='ward')\n",
        "y_agg = agg.fit_predict(X_pca)\n",
        "\n",
        "# 5. Visualize clusters in 2D PCA space\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_agg, cmap='viridis', s=50, alpha=0.7, edgecolor='k')\n",
        "plt.title('Agglomerative Clustering on PCA-Reduced Breast Cancer Data')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AyBB1wIZvzIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43.Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN side-by-side?"
      ],
      "metadata": {
        "id": "KdVO6EJFv0tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Generate noisy circular data\n",
        "X, y_true = make_circles(n_samples=500, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "# 2. Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "# 3. Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.15, min_samples=5)\n",
        "y_dbscan = dbscan.fit_predict(X)\n",
        "\n",
        "# 4. Visualize both results side-by-side\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# K-Means plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', s=50, edgecolor='k')\n",
        "plt.title(\"K-Means Clustering on make_circles Data\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "\n",
        "# DBSCAN plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='viridis', s=50, edgecolor='k')\n",
        "plt.title(\"DBSCAN Clustering on make_circles Data\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hZTYp-eiv9SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering?"
      ],
      "metadata": {
        "id": "LVYxNdFIwBIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "\n",
        "# 2. Apply K-Means clustering (3 clusters for 3 iris species)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "# 3. Compute silhouette values\n",
        "silhouette_vals = silhouette_samples(X, y_kmeans)\n",
        "overall_silhouette = silhouette_score(X, y_kmeans)\n",
        "\n",
        "# 4. Plot silhouette coefficients for each sample\n",
        "plt.figure(figsize=(8, 6))\n",
        "y_lower = 10  # For spacing between clusters\n",
        "\n",
        "for i in range(3):\n",
        "    cluster_silhouette_vals = silhouette_vals[y_kmeans == i]\n",
        "    cluster_silhouette_vals.sort()\n",
        "    cluster_size = cluster_silhouette_vals.shape[0]\n",
        "    y_upper = y_lower + cluster_size\n",
        "\n",
        "    plt.fill_betweenx(\n",
        "        np.arange(y_lower, y_upper),\n",
        "        0,\n",
        "        cluster_silhouette_vals,\n",
        "        alpha=0.7,\n",
        "        label=f\"Cluster {i + 1}\"\n",
        "    )\n",
        "    y_lower = y_upper + 10  # Add spacing between clusters\n",
        "\n",
        "# Draw average silhouette score line\n",
        "plt.axvline(x=overall_silhouette, color=\"red\", linestyle=\"--\", label=f\"Avg Silhouette = {overall_silhouette:.2f}\")\n",
        "\n",
        "# Plot settings\n",
        "plt.title(\"Silhouette Plot for K-Means Clustering on Iris Dataset\")\n",
        "plt.xlabel(\"Silhouette Coefficient Values\")\n",
        "plt.ylabel(\"Cluster Label\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hUTzbuYdxwDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45.Generate synthetic data using make_blobs and apply Agglomerative Clustering with 'average' linkage. Visualize clusters?"
      ],
      "metadata": {
        "id": "K92XyZW7xxOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Generate synthetic data\n",
        "X, y_true = make_blobs(\n",
        "    n_samples=400,\n",
        "    centers=4,\n",
        "    cluster_std=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 2. Apply Agglomerative Clustering with 'average' linkage\n",
        "agg = AgglomerativeClustering(n_clusters=4, linkage='average')\n",
        "y_agg = agg.fit_predict(X)\n",
        "\n",
        "# 3. Visualize clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_agg, cmap='viridis', s=50, edgecolor='k')\n",
        "plt.title(\"Agglomerative Clustering with 'Average' Linkage\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "taD3JGa3x-5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "46. Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4 features?"
      ],
      "metadata": {
        "id": "OW18kcydyG_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# 2. Create a DataFrame for easier visualization\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "# 3. Apply K-Means clustering (3 clusters expected for 3 wine types)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df['Cluster'] = kmeans.fit_predict(X)\n",
        "\n",
        "# 4. Use only the first 4 features for pairplot visualization\n",
        "subset_features = feature_names[:4]\n",
        "\n",
        "# 5. Visualize with Seaborn pairplot\n",
        "sns.pairplot(df, vars=subset_features, hue='Cluster', palette='viridis', diag_kind='kde')\n",
        "plt.suptitle(\"K-Means Clustering on Wine Dataset (First 4 Features)\", y=1.02, fontsize=14)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6-Fyz9DryPBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "47.Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the count?"
      ],
      "metadata": {
        "id": "5u-wB-MhyVD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Generate synthetic noisy blob data\n",
        "X, _ = make_blobs(n_samples=500, centers=3, cluster_std=0.8, random_state=42)\n",
        "\n",
        "# Add random noise points\n",
        "rng = np.random.RandomState(42)\n",
        "noise = rng.uniform(low=-10, high=10, size=(30, 2))  # 30 random noise points\n",
        "X_noisy = np.vstack([X, noise])\n",
        "\n",
        "# 2. Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.8, min_samples=5)\n",
        "labels = dbscan.fit_predict(X_noisy)\n",
        "\n",
        "# 3. Count clusters and noise points\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise = list(labels).count(-1)\n",
        "\n",
        "print(f\"Number of clusters found (excluding noise): {n_clusters}\")\n",
        "print(f\"Number of noise points: {n_noise}\")\n",
        "\n",
        "# 4. Visualize results\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_noisy[:, 0], X_noisy[:, 1], c=labels, cmap='viridis', s=50, edgecolor='k')\n",
        "plt.title(\"DBSCAN Clustering with Noisy Blobs\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "29KnFdOdydrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "48.Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the clusters.?"
      ],
      "metadata": {
        "id": "yU_8qCD2yj9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "# 2. Standardize the data for t-SNE\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Reduce dimensions using t-SNE (2 components for visualization)\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, learning_rate=200)\n",
        "X_tsne = tsne.fit_transform(X_scaled)\n",
        "\n",
        "# 4. Apply Agglomerative Clustering\n",
        "agg = AgglomerativeClustering(n_clusters=10, linkage='ward')\n",
        "y_agg = agg.fit_predict(X_tsne)\n",
        "\n",
        "# 5. Visualize the clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_agg, cmap='tab10', s=40, edgecolor='k', alpha=0.7)\n",
        "plt.title(\"Agglomerative Clustering on t-SNE Reduced Digits Data\")\n",
        "plt.xlabel(\"t-SNE Component 1\")\n",
        "plt.ylabel(\"t-SNE Component 2\")\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Zmrpj2H8ysZQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}