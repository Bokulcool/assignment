{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAs-5ahQ3tMP"
      },
      "outputs": [],
      "source": [
        "1. What does R-squared represent in a regression model?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What does R-squared represent in a regression model?\n",
        "\n",
        "R-squared in a regression model represents the proportion of the variance in the dependent variable that is explained by the independent variable(s). It measures how well the regression model fits the observed data, with values ranging from 0 to 1. An R-squared of 1 indicates that the model perfectly explains all the variability of the response data around its mean, while an R-squared of 0 means the model does not explain any of the variability.\n",
        "\n",
        "Essentially, R-squared tells you how much of the change in the outcome variable can be predicted from changes in the predictor variables. For example, an R-squared of 0.6 means that 60% of the variance in the dependent variable is accounted for by the model.\n",
        "\n",
        "However, R-squared does not imply causation and should be considered alongside other statistics, since a high R-squared alone does not guarantee that the model is appropriate or that the predictors have a causal relationship with the outcome.\n",
        "\n",
        "In summary, R-squared is a key goodness-of-fit metric indicating the explanatory power of a regression model, helping to understand how well the model captures the variation in the data."
      ],
      "metadata": {
        "id": "5VLsNwji38L5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 What are the assumptions of linear regression?\n",
        "\n",
        "The key assumptions of linear regression are:\n",
        "\n",
        "Linearity: The relationship between the independent variables and the dependent variable is linear. The expected change in the dependent variable is proportional to changes in the predictors.\n",
        "\n",
        "Independence of Errors: The residuals (errors) are independent of each other. There should be no correlation between consecutive errors, which is especially important in time series data.\n",
        "\n",
        "Homoscedasticity: The variance of the residuals is constant across all levels of the independent variables. In other words, residuals should be evenly spread without forming patterns such as increasing or decreasing variance.\n",
        "\n",
        "Normality of Errors: The residuals are normally distributed. This assumption is important for reliable hypothesis testing and confidence intervals.\n",
        "\n",
        "No Multicollinearity: The independent variables are not highly correlated with each other to avoid redundancy and instability in coefficient estimates.\n",
        "\n",
        "No Perfect Multicollinearity: None of the predictors is a perfect linear combination of others.\n",
        "\n",
        "Correct Model Specification: The model includes all relevant variables and the relationship form (linear) is correct without misspecification.\n",
        "\n",
        "These assumptions ensure that the estimates from the regression are unbiased, consistent, and efficient, and that inferential statistics like p-values are valid. Violation of these assumptions may lead to misleading or invalid conclusions from the regression analysis"
      ],
      "metadata": {
        "id": "sOkxVc-N4E_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What is the difference between R-squared and Adjusted R-squared?\n",
        "\n",
        "\n",
        "R-squared and Adjusted R-squared are both measures of how well a regression model explains the variability of the dependent variable, but they differ in key ways:\n",
        "\n",
        "R-squared measures the proportion of the variance in the dependent variable that is explained by the independent variables in the model. It ranges from 0 to 1 and always increases or stays the same when more predictors are added, regardless of their usefulness. This can lead to an overestimation of model performance if irrelevant predictors are included.\n",
        "\n",
        "Adjusted R-squared adjusts the R-squared value by accounting for the number of predictors relative to the sample size. It penalizes the addition of predictors that do not improve the model significantly, so it can decrease if useless variables are added. This makes adjusted R-squared a more reliable measure for model comparison, especially when models have different numbers of predictors.\n",
        "\n",
        "In summary, R-squared is a straightforward measure of explained variance, while Adjusted R-squared provides a more accurate model fit assessment by balancing explained variance against model complexity. Adjusted R-squared should be preferred when evaluating multiple regression models with different numbers of independent variables."
      ],
      "metadata": {
        "id": "rtrLE6WI4Sv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Why do we use Mean Squared Error (MSE)?\n",
        "\n",
        "Mean Squared Error (MSE) is used because it provides a clear and mathematically convenient measure of the average squared difference between predicted values and actual observed values in regression models. It serves several important purposes:\n",
        "\n",
        "Quantifies Prediction Accuracy: MSE measures how close the predictions are to the actual outcomes, with a lower MSE indicating better model performance.\n",
        "\n",
        "Squares Errors to Penalize Large Deviations: By squaring the differences, MSE emphasizes larger errors more strongly, making models sensitive to outliers and pushing them to reduce big mistakes.\n",
        "\n",
        "Mathematical Convenience: MSE is differentiable and convex, making it suitable for optimization algorithms like least squares and gradient descent to efficiently find the best fit.\n",
        "\n",
        "Standard Metric for Model Comparison: MSE enables comparing different models or different parameter settings quantitatively by assessing which has the smallest averaged squared error.\n",
        "\n",
        "Foundation for Other Metrics: It forms the basis for related metrics such as Root Mean Squared Error (RMSE), which provides error in the original units of the target variable for better interpretability.\n",
        "\n",
        "Overall, MSE is a fundamental and widely used metric in regression analysis and machine learning for evaluating and optimizing predictive accuracy, balancing sensitivity to error magnitude with mathematical tractability.\n",
        "\n"
      ],
      "metadata": {
        "id": "eZFfuOpS4gWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What does an Adjusted R-squared value of 0.85 indicate?\n",
        "\n",
        "An Adjusted R-squared value of 0.85 indicates that, after accounting for the number of predictors in the model, 85% of the variability in the dependent variable is explained by the independent variables included.\n",
        "\n",
        "Unlike the regular R-squared, which can artificially increase as more variables are added regardless of their relevance, Adjusted R-squared penalizes unnecessary predictors. Therefore, a value of 0.85 reflects a strong model fit while suggesting that the included predictors meaningfully contribute to explaining the variation in the outcome without overfitting.\n",
        "\n",
        "In practical terms, this means the model reliably explains a significant portion of the variance, and adding more variables would be unlikely to improve model performance unless they provide substantial additional information."
      ],
      "metadata": {
        "id": "OLS_91_P47it"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.How do we check for normality of residuals in linear regression?\n",
        "\n",
        "Check for normality of residuals in linear regression, you can use the following methods:\n",
        "\n",
        "Visual Inspection with Plots:\n",
        "\n",
        "Histogram of Residuals: Plot a histogram of residuals and compare it to a normal distribution curve overlaid. The closer the histogram matches the bell-shaped curve, the more normal the residuals are.\n",
        "\n",
        "Q-Q Plot (Quantile-Quantile Plot): This plots the quantiles of the residuals against the theoretical quantiles of a normal distribution. If the points align closely along the diagonal line, residuals are approximately normally distributed.\n",
        "\n",
        "Statistical Normality Tests:\n",
        "\n",
        "Shapiro-Wilk Test: A widely used formal test for normality; a high p-value (typically > 0.05) indicates residuals do not significantly deviate from normality.\n",
        "\n",
        "Kolmogorov-Smirnov Test: Another test to compare the distribution of residuals to a normal distribution.\n",
        "\n",
        "Software Tools and Residual Analysis:\n",
        "\n",
        "Many statistical packages (e.g., R, SPSS, Python) provide functions to extract residuals and conduct these tests and plots easily.\n",
        "\n",
        "It is common to check standardized or studentized residuals for normality assessment.\n",
        "\n",
        "While strict normality of residuals may be more important in small samples for valid hypothesis testing, in large samples moderate deviations often have limited impact on regression estimates.\n",
        "\n",
        "In summary, normality of residuals is assessed through graphical plots and formal tests to ensure regression inference is valid.‚Äã"
      ],
      "metadata": {
        "id": "6TNjBIXq5KPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.What is multicollinearity, and how does it impact regression?\n",
        "\n",
        "Multicollinearity refers to a situation in a regression model where two or more independent variables are highly correlated with each other. This means that one predictor variable can be linearly predicted from the others with a substantial degree of accuracy, creating redundancy among variables.\n",
        "\n",
        "Impact on Regression:\n",
        "\n",
        "It causes instability in the estimation of regression coefficients, making them unreliable and highly sensitive to small changes in the data.\n",
        "\n",
        "The standard errors of the coefficients increase, leading to wider confidence intervals and less statistically significant predictors.\n",
        "\n",
        "It becomes difficult to interpret the individual effect of each predictor because their influences overlap.\n",
        "\n",
        "Although the overall model may still predict well, the precision and interpretability of individual coefficient estimates are compromised.\n",
        "\n",
        "Perfect multicollinearity (exact linear dependence) prevents the matrix inversion required for ordinary least squares estimation, making coefficient estimation impossible.\n",
        "\n",
        "Multicollinearity can be detected using measures like the Variance Inflation Factor (VIF) or tolerance, and mitigated by techniques such as removing or combining correlated predictors, or by applying regularization methods.\n",
        "\n",
        "In summary, multicollinearity reduces the reliability and clarity of regression analysis results by causing redundancy among predictors and inflating estimation errors."
      ],
      "metadata": {
        "id": "-LmZvIUi5dgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.What is Mean Absolute Error (MAE)?\n",
        "\n",
        "\n",
        "Mean Absolute Error (MAE) is a widely used metric for evaluating the performance of regression models. It measures the average magnitude of the errors (the difference between predicted and actual values) in a set of predictions, without considering their direction.\n",
        "MAE=1/n ‚àë_(i=1)^n‚à£y_i-y ÃÇ_i‚à£\n",
        "\n",
        "where:\n",
        "\tn= number of observations\n",
        "\ty_i= actual (true) value\n",
        "\ty ÃÇ_i= predicted value\n",
        "\t‚à£y_i-y ÃÇ_i‚à£= absolute difference between actual and predicted value\n",
        "\n"
      ],
      "metadata": {
        "id": "4T9W9Qes5uQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.What are the benefits of using an ML pipeline?\n",
        "\n",
        "The benefits of using a Machine Learning (ML) pipeline include:\n",
        "\n",
        "Reproducibility: ML pipelines ensure that models give consistent results by defining a standardized sequence of steps and parameters. This allows easy comparison between different models and consistent outcomes over time.\n",
        "\n",
        "Efficiency: Automating routine tasks such as data preprocessing, feature engineering, and model training saves time and reduces human errors, accelerating development cycles and boosting productivity.\n",
        "\n",
        "Scalability: Pipelines accommodate growing data volumes and increasing model complexity without requiring extensive rework. They enable smooth integration of new data sources, algorithms, and changing project needs.\n",
        "\n",
        "Ease of Deployment: By structuring the workflow from data collection to model deployment, ML pipelines simplify moving models into production environments, shortening time to market and enabling continuous delivery.\n",
        "\n",
        "Collaboration: Pipelines create structured, documented workflows that enhance team collaboration by clarifying each member‚Äôs contributions and effects of changes, reducing miscommunication and errors.\n",
        "\n",
        "Modularity and Flexibility: Components of pipelines can be reused, replaced, or updated independently, facilitating experimentation and optimization without starting from scratch.\n",
        "\n",
        "Error Reduction and Quality Assurance: Validation and logging at each stage help catch errors early, improving model robustness and quality.\n",
        "\n",
        "In summary, ML pipelines streamline, standardize, and automate the entire machine learning lifecycle, enabling teams to build reliable, scalable, and maintainable models faster and more collaboratively"
      ],
      "metadata": {
        "id": "vgEq-eD79vSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.Why is RMSE considered more interpretable than MSE?\n",
        "\n",
        "RMSE (Root Mean Squared Error) is considered more interpretable than MSE (Mean Squared Error) because RMSE is expressed in the same units as the target variable, while MSE is in squared units.\n",
        "\n",
        "To explain more clearly:\n",
        "\n",
        "MSE calculates the average of the squared differences between actual and predicted values, which causes the error measure to be in the units squared of the original data (e.g., if the target is in meters, MSE is in square meters). This can make it harder to directly understand the scale of the error.\n",
        "\n",
        "RMSE takes the square root of MSE, converting the error back into the original data units (meters in this example), making it more intuitive and easier to relate to since it represents the average prediction error magnitude.\n",
        "\n",
        "Thus, RMSE provides a more natural and meaningful measure to communicate model performance and error magnitude in practical terms.\n",
        "\n",
        "Additionally, both metrics penalize larger errors but RMSE's unit consistency often makes it the preferred metric for interpreting and comparing predictive accuracy."
      ],
      "metadata": {
        "id": "ofkcqfpb9-QG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.What is pickling in Python, and how is it useful in ML?\n",
        "\n",
        "Pickling in Python is the process of converting a Python object hierarchy into a byte stream, which can be saved to a file or transferred over a network. This process is also known as serialization. The byte stream created during pickling contains all the necessary information to recreate the original object, including its state, variables, and attributes. Unpickling is the inverse process, where the byte stream is converted back into the original Python object.\n",
        "\n",
        "In machine learning, pickling is particularly useful because it allows saving trained models to disk after the computationally expensive training process is complete. These saved models can be quickly loaded later for inference without retraining, enabling easy deployment and reproducibility across different sessions or environments. It also helps in sharing models between collaborators or integrating them into applications.\n",
        "\n",
        "Python‚Äôs pickle module provides functions like dump() for serializing objects to a file and load() for deserializing objects back into Python. This makes the process straightforward and efficient.\n",
        "\n",
        "However, it's important to note that unpickling data from untrusted sources can be insecure as the process can execute arbitrary code. Therefore, pickled data should only be loaded from trusted sources.\n",
        "\n",
        "In summary, pickling simplifies saving, loading, and sharing complex Python objects such as ML models, making it an essential tool for efficient ML workflows"
      ],
      "metadata": {
        "id": "IFDAszeR-J_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12 What does a high R-squared value mean?\n",
        "\n",
        "A high R-squared value in a regression model means that a large proportion of the variance in the dependent variable is explained by the independent variable(s). In other words, the model fits the data well, and the predictions closely follow the observed data points.\n",
        "\n",
        "For example, an R-squared of 0.85 indicates that 85% of the variability in the outcome can be explained by the model‚Äôs predictors. The closer the value is to 1, the better the fit, implying less unexplained variation or noise.\n",
        "\n",
        "However, a high R-squared does not guarantee that the model is the correct or best model. It can sometimes indicate overfitting, especially when many predictors are used, or when irrelevant variables inflate the R-squared without improving predictive usefulness. Therefore, R-squared values should be interpreted alongside other diagnostic measures and domain knowledge.\n",
        "\n",
        "In summary, a high R-squared value generally reflects a strong explanatory power of the regression model regarding the variance in the response variable, but it is not a definitive measure of model validity on its own."
      ],
      "metadata": {
        "id": "BsrpEslf-8gO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.What happens if linear regression assumptions are violated?\n",
        "\n",
        "\n",
        "If the assumptions of linear regression are violated, it can lead to various problems affecting the reliability, accuracy, and interpretability of the model:\n",
        "\n",
        "Bias and Inconsistency: Violations of linearity or independence can cause coefficient estimates to be biased or inconsistent, meaning they do not accurately reflect the true relationships or fail to converge to true values as sample size increases.\n",
        "\n",
        "Inefficiency: Ordinary Least Squares (OLS) estimators may become inefficient if assumptions like homoscedasticity (constant variance of residuals) are violated, leading to less precise estimates with larger standard errors.\n",
        "\n",
        "Unreliable Inference: P-values and confidence intervals may become invalid or misleading, especially if residuals are not normally distributed or if there is autocorrelation, leading to incorrect hypothesis testing conclusions.\n",
        "\n",
        "Poor Predictive Performance: When assumptions do not hold, the model may perform poorly on new data, reducing generalizability and accuracy.\n",
        "\n",
        "Erroneous Standard Errors: Heteroscedasticity and autocorrelation cause standard error estimates to be biased, affecting tests of significance and intervals.\n",
        "\n",
        "Multicollinearity: Although it doesn't bias estimates, multicollinearity increases variance of coefficients, making them unstable and difficult to interpret.\n",
        "\n",
        "Model Misspecification: Ignoring necessary variables or incorrect functional form (non-linearity) can result in an inadequate model fit and misleading results.\n",
        "\n",
        "In practice, mild violations might be tolerated depending on context, but severe or multiple violations necessitate remedial measures like variable transformations, adding missing predictors, using robust regression methods, or alternative estimation techniques like Generalized Least Squares.\n",
        "\n",
        "Overall, violation of assumptions undermines trustworthiness and validity of regression results, making assumption checking a critical step in regression analysis\n"
      ],
      "metadata": {
        "id": "AfV2RP6k_ISf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.How can we address multicollinearity in regression?\n",
        "\n",
        "To address multicollinearity in regression analysis, several effective strategies are commonly used:\n",
        "\n",
        "Remove Highly Correlated Predictors: Analyze correlation matrices or Variance Inflation Factor (VIF) scores to identify and remove one of the variables that are highly correlated. This reduces redundancy and improves model stability.\n",
        "\n",
        "Combine Variables: Use techniques like Principal Component Analysis (PCA) or factor analysis to combine correlated variables into a single composite predictor. This reduces dimensionality but can make interpretation harder.\n",
        "\n",
        "Regularization Methods: Apply Ridge Regression or LASSO which add penalty terms during model fitting to shrink coefficient estimates. Ridge regression reduces multicollinearity by shrinking coefficients of correlated variables, while LASSO can perform variable selection by setting some coefficients exactly to zero.\n",
        "\n",
        "Increase Sample Size: When possible, increasing the sample size can help reduce the effects of multicollinearity by better distinguishing the independent effects.\n",
        "\n",
        "Careful Variable Selection: Avoid including variables that are linear combinations of others and perform domain-based feature selection.\n",
        "\n",
        "Each of these approaches helps mitigate the negative impact of multicollinearity, leading to more reliable coefficient estimates and better model interpretability.\n",
        "\n",
        "For example, applying Ridge Regression often leads to more stable predictions and improved model fit compared to standard linear regression in the presence of high multicollinearity as seen by reduced Variance Inflation Factors and improved performance metrics.\n",
        "\n",
        "In summary, understanding and addressing multicollinearity is crucial for robust regression modeling. Depending on the context, one or a combination of these methods can be employed to ensure the model provides meaningful insights"
      ],
      "metadata": {
        "id": "fIwgToYA_XRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How can feature selection improve model performance in regression analysis?\n",
        "\n",
        "Feature selection improves model performance in regression analysis by identifying and retaining the most relevant features while eliminating irrelevant or redundant ones. This benefits the model in several ways:\n",
        "\n",
        "Reduces Overfitting: By removing noisy, irrelevant features, the model is less likely to fit the noise in the training data, improving generalization to new data.\n",
        "\n",
        "Simplifies the Model: Fewer features lead to a simpler model that is easier to interpret and understand.\n",
        "\n",
        "Enhances Prediction Accuracy: Focusing on important features often improves the model‚Äôs predictive accuracy by reducing variance and bias.\n",
        "\n",
        "Speeds Up Training: With fewer features, the computational cost of training the model decreases, making it faster and more efficient.\n",
        "\n",
        "Mitigates Multicollinearity: Feature selection helps avoid highly correlated predictors that can destabilize coefficient estimates.\n",
        "\n",
        "Common techniques for feature selection in regression include correlation analysis, univariate statistical tests, recursive feature elimination (RFE), Lasso regression for embedded feature selection, and feature importance rankings from tree-based models. These methods help identify the features most predictive of the target variable to build robust, interpretable, and efficient regression models\n"
      ],
      "metadata": {
        "id": "i-c9t9aF_oPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. How is Adjusted R-squared calculated?\n",
        "\n",
        "Adjusted R-squared is calculated using the formula:\n",
        "\"Adjusted \" R^2=1-(1-R^2)√ó(n-1)/(n-p-1)\n",
        "\n",
        "where:\n",
        "\tR^2 is the original R-squared value of the model,\n",
        "\n",
        "\tn is the number of observations (data points),\n",
        "\n",
        "\tp is the number of predictors (independent variables) in the model.\n",
        "\n",
        "This formula adjusts the R-squared by penalizing the addition of variables that do not improve the model sufficiently. The numerator n-1 accounts for the degrees of freedom associated with the sample size, and the denominator n-p-1 adjusts for the number of predictors and includes the intercept term.\n",
        "\n",
        "This adjustment ensures that adding more predictors which do not add explanatory power will decrease the Adjusted R-squared, making it a better measure than regular R-squared for comparing models with different numbers of predictors.\n",
        "\n",
        "For example, if a model has 30 observations, 5 predictors, and an R-squared value of 0.8, the Adjusted R-squared is calculated by plugging these values into the formula.\n",
        "\n",
        "This metric helps in understanding how well the model explains the variance in the dependent variable while accounting for model complexity\n"
      ],
      "metadata": {
        "id": "c4YF9WKb_4of"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.Why is MSE sensitive to outliers?\n",
        "\n",
        "Mean Squared Error (MSE) is sensitive to outliers because the errors (differences between predicted and actual values) are squared before being averaged. Squaring the errors magnifies the impact of larger deviations disproportionately compared to smaller ones. This means that even a single large outlier can cause a significant increase in the MSE value, making the metric heavily influenced by extreme errors. This sensitivity often results in MSE reflecting poorer model performance when outliers are present, because the large squared errors dominate the average calculation.\n",
        "\n",
        "In contrast, metrics like Mean Absolute Error (MAE) do not square the errors and thus treat all deviations linearly, making them less sensitive to outliers. The squaring characteristic of MSE is useful when penalizing larger errors more strongly is desired, but it also means MSE is less robust to noisy data or outliers"
      ],
      "metadata": {
        "id": "s9-suR50Aay-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the role of homoscedasticity in linear regression?\n",
        "\n",
        "Homoscedasticity in linear regression refers to the assumption that the variance of the error terms (residuals) is constant across all levels of the independent variable(s). This means the spread or \"scatter\" of the errors around the regression line does not systematically change as the predictor variable values change. When this assumption holds, the model's errors are evenly distributed over the range of predicted values.\n",
        "\n",
        "The role of homoscedasticity is crucial because it ensures that:\n",
        "\n",
        "The coefficient estimates are efficient and unbiased with minimum variance.\n",
        "\n",
        "Statistical tests, such as hypothesis tests on coefficients and confidence intervals, are valid and reliable.\n",
        "\n",
        "The predictive accuracy and reliability of the model remain consistent across the entire range of predictor values.\n",
        "\n",
        "If homoscedasticity is violated (a condition known as heteroscedasticity), the variance of errors differs at different levels of the predictor variable(s), potentially leading to inefficient estimates and invalid inference, diminishing confidence in the model's results.\n",
        "\n",
        "In summary, homoscedasticity is a core assumption of linear regression that helps guarantee the robustness, efficiency, and validity of model estimation and inference"
      ],
      "metadata": {
        "id": "XHLHid-9AlEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.What is Root Mean Squared Error (RMSE)?\n",
        "\n",
        "Root Mean Squared Error (RMSE) is a widely used metric in regression analysis that measures the average magnitude of the errors between predicted values and actual observed values. It is defined as the square root of the average of the squared differences between predicted and actual values.\n",
        "Mathematically, RMSE is calculated as:\n",
        "\"RMSE\"=‚àö(1/n ‚àë_(i=1)^n(y_i-y ÃÇ_i )^2 )\n",
        "\n",
        "where:\n",
        "\n",
        "\ty_i is the actual value for the i-th observation,\n",
        "\n",
        "\ty ÃÇ_i is the predicted value for the i-th observation,\n",
        "\n",
        "\tn is the total number of observations.\n",
        "\n",
        "RMSE provides the error measure in the same units as the target variable, making it more interpretable than metrics like Mean Squared Error (MSE) that are in squared units. A lower RMSE value indicates better model performance, implying predictions are closer to actual values.\n",
        "\n",
        "RMSE is sensitive to large errors due to the squaring step, thus heavily penalizing large deviations between predicted and actual values, which helps emphasize minimizing big prediction mistakes.\n",
        "\n",
        "In summary, RMSE quantifies prediction accuracy by capturing the standard deviation of prediction errors, providing a clear, intuitive measure of how well a regression model fits the data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gbn-rOuQAviI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Why is pickling considered risky?\n",
        "\n",
        "Pickling is considered risky primarily due to potential health hazards related to contamination and the presence of harmful compounds. Some key risks include:\n",
        "\n",
        "Microbial Contamination: Improper pickling or storage can lead to growth of harmful microorganisms such as fungi or bacteria, causing food spoilage and foodborne illnesses. For example, botulism is a serious risk if pickling conditions are unsafe.\n",
        "\n",
        "Carcinogenic Compounds: Certain pickled foods, especially traditionally fermented vegetables, have been associated with increased risks of cancers (e.g., stomach, esophageal) due to the formation of carcinogenic compounds like nitrosamines during fermentation.\n",
        "\n",
        "Toxins Production: Some fungi associated with pickles can produce toxins (mycotoxins like fumonisins) that are harmful to human health, potentially causing liver or kidney damage.\n",
        "\n",
        "Improper Ingredients or Equipment: Use of unsuitable salts, metals (like copper), or artificial coloring agents during pickling can introduce toxic substances.\n",
        "\n",
        "Chemical Additives: Some additives or preservatives used to enhance shelf life or aesthetics may have adverse health effects if not properly regulated.\n",
        "\n",
        "These risks emphasize the importance of following safe pickling practices, including using proper vinegar acidity, hygiene, suitable materials, and storage conditions to ensure food safety.\n",
        "\n",
        "In summary, although pickling preserves food and adds flavor, unsafe preparation and storage practices pose health risks that require attention and careful handling"
      ],
      "metadata": {
        "id": "N93JYbOGCQZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What alternatives exist to pickling for saving ML models?\n",
        "\n",
        "Alternatives to pickling for saving machine learning models include:\n",
        "\n",
        "Joblib: A Python library optimized for serializing large numpy arrays efficiently. It works similarly to pickle but is faster and more efficient for numerical data, making it popular for scikit-learn models.\n",
        "\n",
        "ONNX (Open Neural Network Exchange): An open format designed for interoperable ML model saving, supporting models from many frameworks including PyTorch, TensorFlow, and others. It allows models to be used across different environments and frameworks.\n",
        "\n",
        "HDF5 (Hierarchical Data Format version 5): Used especially with Keras and TensorFlow, it is good for storing large models and datasets efficiently, supporting partial I/O.\n",
        "\n",
        "Protocol Buffers (protobuf): A compact, language-neutral binary format often used by TensorFlow for saving models in the SavedModel format.\n",
        "\n",
        "Framework-specific methods:\n",
        "\n",
        "PyTorch uses .pt or state dictionaries saved with torch.save.\n",
        "\n",
        "TensorFlow/Keras use SavedModel or .h5 formats.\n",
        "\n",
        "Java frameworks may use POJO or MOJO formats.\n",
        "\n",
        "JSON or YAML: Useful for saving model metadata or simple model parameters in a human-readable format, often combined with binary weight files.\n",
        "\n",
        "These alternatives provide benefits like better cross-language support, improved security compared to pickle, efficiency in storage/loading, and better integration in deployment pipelines.\n",
        "\n",
        "In summary, pickling is convenient and common in Python, but for broader compatibility, security, and efficiency, libraries like Joblib and standardized formats like ONNX or framework-specific save/load methods are preferred when saving ML models.\n",
        "\n"
      ],
      "metadata": {
        "id": "BZXQPohECdc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. What is heteroscedasticity, and why is it a problem?\n",
        "\n",
        "Heteroscedasticity is a condition in regression analysis where the variance of the residuals (errors) is not constant across all levels of the independent variable(s). In other words, the spread or dispersion of the residuals changes depending on the value of the predictors, often visible as a ‚Äúfan-shaped‚Äù or cone-shaped pattern in plots of residuals against fitted values.\n",
        "\n",
        "Why is heteroscedasticity a problem?\n",
        "\n",
        "Violates Regression Assumptions: It violates the assumption of homoscedasticity (constant variance of residuals), which is critical for ordinary least squares (OLS) regression to produce efficient and unbiased estimates of standard errors.\n",
        "\n",
        "Inefficient Estimates: While coefficient estimates remain unbiased, they become inefficient, producing larger standard errors.\n",
        "\n",
        "Invalid Inference: Hypothesis tests, confidence intervals, and p-values become unreliable, potentially leading to incorrect conclusions about predictor significance.\n",
        "\n",
        "Misleading Goodness-of-Fit: It can distort measures like R-squared, overstating model fit or predictive accuracy.\n",
        "\n",
        "In practice, heteroscedasticity is common, especially in datasets with wide-ranging values or outliers. Detecting it involves residual plots or tests like Breusch-Pagan or White tests. Remedies include transforming variables, weighted least squares regression, or robust standard errors.\n",
        "\n",
        "In summary, heteroscedasticity indicates unequal error variance across predictors, adversely affecting the reliability of regression inference and requiring corrective measures to ensure valid model conclusions."
      ],
      "metadata": {
        "id": "NF7WkgTTCtv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.How can interaction terms enhance a regression model's predictive power?\n",
        "\n",
        "Interaction terms enhance a regression model's predictive power by allowing the effect of one independent variable on the dependent variable to depend on the value of another independent variable. In other words, interaction terms capture joint or combined effects of variables that are not explained by their individual contributions alone.\n",
        "\n",
        "By including interaction terms‚Äîtypically created as the product of two or more predictors‚Äîthe model becomes more flexible and can represent situations where the relationship between a predictor and the outcome changes based on the level of another predictor. This allows the model to better fit complex patterns in data, improving accuracy and explanatory power.\n",
        "\n",
        "For example, in real estate price modeling, the effect of apartment size on price might differ depending on whether it is located in the city center or not. Without interaction terms, the model assumes a constant effect of size regardless of location. By adding an interaction between size and location, the model reflects different slopes for city center vs suburban apartments.\n",
        "\n",
        "Including interaction terms often leads to improved predictive performance because it:\n",
        "\n",
        "Captures nuanced relationships in the data,\n",
        "\n",
        "Reflects real-world joint effects between variables,\n",
        "\n",
        "Helps reveal conditional dependencies that simple additive models miss.\n",
        "\n",
        "However, interaction terms also increase model complexity and require careful interpretation, as coefficients represent conditional effects rather than simple independent contributions.\n",
        "\n",
        "In summary, interaction terms allow regression models to account for joint influences of variables on the target, leading to richer, more accurate modeling of complex phenomena"
      ],
      "metadata": {
        "id": "yZsacs6dC7OG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                              Practicle Question"
      ],
      "metadata": {
        "id": "h3FMaNnZDNr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python script to visualize the distribution of errors (residuals) for a multiple linear regression model\n",
        "using Seaborn's \"diamonds\" dataset"
      ],
      "metadata": {
        "id": "Ho0DI72JFNIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1. Write a Python script to visualize the distribution of errors (residuals)\n",
        "for a multiple linear regression model using Seaborn's \"diamonds\" dataset.\n",
        "\n",
        "#  Import necessary libraries\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "#  Load the diamonds dataset\n",
        "diamonds = sns.load_dataset(\"diamonds\")\n",
        "\n",
        "#  Select numerical features for regression\n",
        "# We'll predict 'price' using 'carat', 'depth', and 'table'\n",
        "df = diamonds[['price', 'carat', 'depth', 'table']].dropna()\n",
        "\n",
        "#  Define features (X) and target (y)\n",
        "X = df[['carat', 'depth', 'table']]\n",
        "y = df['price']\n",
        "\n",
        "#  Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#  Create and fit the multiple linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#  Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#  Calculate residuals (errors)\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "#  Visualize the distribution of residuals\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(residuals, bins=40, kde=True, color='royalblue')\n",
        "plt.title(\"Distribution of Residuals (Errors) ‚Äî Diamonds Dataset\", fontsize=14)\n",
        "plt.xlabel(\"Residuals (Actual - Predicted)\", fontsize=12)\n",
        "plt.ylabel(\"Frequency\", fontsize=12)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "Explanation:\n",
        "\n",
        "Load Dataset: Uses Seaborn‚Äôs built-in diamonds dataset.\n",
        "\n",
        "Select Variables: Predicts price using numeric predictors (carat, depth, and table).\n",
        "\n",
        "Train Model: Fits a Multiple Linear Regression using scikit-learn.\n",
        "\n",
        "Compute Residuals: Difference between actual and predicted values.\n",
        "\n",
        "Plot Residual Distribution: Uses sns.histplot() with a KDE curve to visualize\n",
        " how residuals are distributed."
      ],
      "metadata": {
        "id": "iwUOGKtXDRXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a Python script to calculate and print Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root\n",
        "Mean Squared Error (RMSE) for a linear regression model."
      ],
      "metadata": {
        "id": "d_Q1xZW3FIVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "#  Load the diamonds dataset from Seaborn\n",
        "diamonds = sns.load_dataset(\"diamonds\")\n",
        "\n",
        "# Select numerical columns for simplicity\n",
        "# We'll predict 'price' using 'carat', 'depth', and 'table'\n",
        "df = diamonds[['price', 'carat', 'depth', 'table']].dropna()\n",
        "\n",
        "#  Define features (X) and target (y)\n",
        "X = df[['carat', 'depth', 'table']]\n",
        "y = df['price']\n",
        "\n",
        "#  Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#  Train the Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#  Make predictions on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#  Calculate evaluation metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Print the results\n",
        "print(\" Model Performance Metrics:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n"
      ],
      "metadata": {
        "id": "Md95DWzwEQj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a Python script to check if the assumptions of linear regression are met. Use a scatter plot to check linearity, residuals plot for homoscedasticity, and correlation matrix for multicollinearity."
      ],
      "metadata": {
        "id": "TTXDF5PyFSd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìò Import required libraries\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# üßÆ Load the diamonds dataset\n",
        "diamonds = sns.load_dataset(\"diamonds\")\n",
        "\n",
        "# üßπ Select relevant numerical features\n",
        "df = diamonds[['price', 'carat', 'depth', 'table']].dropna()\n",
        "\n",
        "# ‚úÖ Define features (X) and target (y)\n",
        "X = df[['carat', 'depth', 'table']]\n",
        "y = df['price']\n",
        "\n",
        "# ‚úÇÔ∏è Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ‚öôÔ∏è Train the Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#  Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#  Calculate residuals\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "#  1. Check Linearity ‚Äî Scatter plot of actual vs. predicted values\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.scatterplot(x=y_test, y=y_pred, alpha=0.5, color='royalblue')\n",
        "plt.title(\"Linearity Check: Actual vs. Predicted Values\", fontsize=14)\n",
        "plt.xlabel(\"Actual Price\")\n",
        "plt.ylabel(\"Predicted Price\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Interpretation:\n",
        "# If points lie roughly along a straight diagonal line ‚Üí linearity assumption holds.\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "#  2. Check Homoscedasticity ‚Äî Residuals vs. Predicted values\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.scatterplot(x=y_pred, y=residuals, alpha=0.5, color='darkorange')\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.title(\"Homoscedasticity Check: Residuals vs. Predicted Values\", fontsize=14)\n",
        "plt.xlabel(\"Predicted Price\")\n",
        "plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Interpretation:\n",
        "# If residuals are randomly scattered around 0 (no funnel or cone shape) ‚Üí constant variance assumption holds.\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "#  3. Check Multicollinearity ‚Äî Correlation matrix among predictors\n",
        "plt.figure(figsize=(6,5))\n",
        "corr = X.corr()\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Correlation Matrix ‚Äî Checking Multicollinearity\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# Interpretation:\n",
        "# Correlation coefficients close to ¬±1 between predictors indicate multicollinearity.\n",
        "# Values below 0.7 generally mean multicollinearity is not a concern.\n",
        "# ----------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "oos0v6fXFbBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python script that creates a machine learning pipeline with feature scaling and evaluates the performance of different regression models"
      ],
      "metadata": {
        "id": "0hDLnyaUFg5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import required libraries\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "#  Load the diamonds dataset\n",
        "diamonds = sns.load_dataset(\"diamonds\")\n",
        "\n",
        "#  Select numeric features for regression\n",
        "# We'll predict 'price' using 'carat', 'depth', and 'table'\n",
        "df = diamonds[['price', 'carat', 'depth', 'table']].dropna()\n",
        "\n",
        "#  Define features (X) and target (y)\n",
        "X = df[['carat', 'depth', 'table']]\n",
        "y = df['price']\n",
        "\n",
        "#  Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#  Define regression models for comparison\n",
        "models = {\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Ridge Regression\": Ridge(alpha=1.0),\n",
        "    \"Lasso Regression\": Lasso(alpha=0.1),\n",
        "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
        "    \"Random Forest\": RandomForestRegressor(random_state=42, n_estimators=100)\n",
        "}\n",
        "\n",
        "#  Evaluate each model using a pipeline with feature scaling\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Create a pipeline with scaling and regression model\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('regressor', model)\n",
        "    ])\n",
        "\n",
        "    # Train the model\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Store results\n",
        "    results.append([name, mae, mse, rmse, r2])\n",
        "\n",
        "# Create a DataFrame to display results\n",
        "results_df = pd.DataFrame(results, columns=[\"Model\", \"MAE\", \"MSE\", \"RMSE\", \"R¬≤\"])\n",
        "\n",
        "#  Display model performance\n",
        "print(\" Model Performance Comparison:\")\n",
        "print(results_df.sort_values(by=\"R¬≤\", ascending=False).reset_index(drop=True))\n"
      ],
      "metadata": {
        "id": "B9nwYHgLFnvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5.Implement a simple linear regression model on a dataset and print the model's coefficients, intercept, and R-squared score."
      ],
      "metadata": {
        "id": "Q5fC6bGQF0vI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#  Load the diamonds dataset\n",
        "diamonds = sns.load_dataset(\"diamonds\")\n",
        "\n",
        "# üßπ Select one feature (e.g., 'carat') and target ('price') for simple linear regression\n",
        "df = diamonds[['carat', 'price']].dropna()\n",
        "\n",
        "#  Define feature (X) and target (y)\n",
        "X = df[['carat']]   # independent variable\n",
        "y = df['price']     # dependent variable\n",
        "\n",
        "#  Split dataset into training and testing sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#  Initialize and train the Simple Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#  Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#  Evaluate the model\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print model parameters and performance\n",
        "print(\" Simple Linear Regression Results\")\n",
        "print(f\"Coefficient (Slope): {model.coef_[0]:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_:.4f}\")\n",
        "print(f\"R¬≤ Score: {r2:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "chQAINzbF8tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Write a Python script that analyzes the relationship between total bill and tip in the 'tips' dataset usingsimple linear regression and visualizes the results?"
      ],
      "metadata": {
        "id": "fMAaMj12Gssv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the tips dataset\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "\n",
        "# Define feature (X) and target (y)\n",
        "X = tips[['total_bill']]   # Independent variable\n",
        "y = tips['tip']            # Dependent variable\n",
        "\n",
        "#  Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Simple Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#  Evaluate model performance\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "#  Print model coefficients and R¬≤ score\n",
        "print(\"Simple Linear Regression on 'tips' Dataset\")\n",
        "print(f\"Coefficient (Slope): {model.coef_[0]:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_:.4f}\")\n",
        "print(f\"R¬≤ Score: {r2:.4f}\")\n",
        "\n",
        "#  Visualize the regression line\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x='total_bill', y='tip', data=tips, color='blue', alpha=0.6, label='Actual data')\n",
        "plt.plot(tips['total_bill'], model.predict(tips[['total_bill']]), color='red', label='Regression Line')\n",
        "plt.title(\"Simple Linear Regression: Total Bill vs Tip\")\n",
        "plt.xlabel(\"Total Bill ($)\")\n",
        "plt.ylabel(\"Tip ($)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1RQ8NnxlGi-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python script that fits a linear regression model to a synthetic dataset with one feature. Use the model to predict new values and plot the data points along with the regression line"
      ],
      "metadata": {
        "id": "u598Jq65G0qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#  Generate synthetic dataset\n",
        "# X values between 0 and 10\n",
        "np.random.seed(42)\n",
        "X = 10 * np.random.rand(50, 1)\n",
        "\n",
        "# y = 2.5x + noise\n",
        "y = 2.5 * X + np.random.randn(50, 1) * 2.0\n",
        "\n",
        "#  Create and train the Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "#  Predict values for new X range (for smooth regression line)\n",
        "X_new = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "y_pred = model.predict(X_new)\n",
        "\n",
        "#  Print model parameters\n",
        "print(\" Linear Regression on Synthetic Data\")\n",
        "print(f\"Coefficient (Slope): {model.coef_[0][0]:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_[0]:.4f}\")\n",
        "\n",
        "#  Plot data points and regression line\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X, y, color='blue', label='Data Points', alpha=0.7)\n",
        "plt.plot(X_new, y_pred, color='red', linewidth=2, label='Regression Line')\n",
        "plt.title(\"Linear Regression on Synthetic Dataset\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bTAgr1KiG5Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python script that pickles a trained linear regression model and saves it to a file."
      ],
      "metadata": {
        "id": "pqLdGu52HgAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import necessary libraries\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#  Generate a small synthetic dataset\n",
        "np.random.seed(42)\n",
        "X = 10 * np.random.rand(50, 1)\n",
        "y = 2.5 * X + np.random.randn(50, 1) * 2.0\n",
        "\n",
        "#  Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#  Train the Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#  Display model parameters\n",
        "print(\"Trained Linear Regression Model\")\n",
        "print(f\"Coefficient (Slope): {model.coef_[0][0]:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_[0]:.4f}\")\n",
        "\n",
        "# Save (pickle) the trained model to a file\n",
        "filename = \"linear_regression_model.pkl\"\n",
        "with open(filename, \"wb\") as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "print(f\"\\nModel has been pickled and saved as '{filename}'\")\n"
      ],
      "metadata": {
        "id": "9IMVBRbPHi2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Write a Python script that fits a polynomial regression model (degree 2) to a dataset and plots the regression curve ?"
      ],
      "metadata": {
        "id": "sKEPq1V-HswP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "#  Generate a synthetic dataset\n",
        "np.random.seed(42)\n",
        "X = np.linspace(0, 10, 50).reshape(-1, 1)\n",
        "y = 3 + 2 * X + 1.5 * X**2 + np.random.randn(50, 1) * 5  # Quadratic relationship with noise\n",
        "\n",
        "#  Create polynomial features (degree = 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "#  Fit the polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "#  Predict values for plotting the regression curve\n",
        "X_new = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "X_new_poly = poly.transform(X_new)\n",
        "y_pred = model.predict(X_new_poly)\n",
        "\n",
        "#  Print model coefficients\n",
        "print(\" Polynomial Regression Model (Degree 2)\")\n",
        "print(f\"Intercept: {model.intercept_[0]:.4f}\")\n",
        "print(f\"Coefficients: {model.coef_[0]}\")\n",
        "\n",
        "#  Plot data points and regression curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X, y, color='blue', label='Data Points', alpha=0.6)\n",
        "plt.plot(X_new, y_pred, color='red', linewidth=2, label='Polynomial Regression Curve')\n",
        "plt.title(\"Polynomial Regression (Degree 2)\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "exAW7r8NHyQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Generate synthetic data for simple linear regression (use random values for X and y) and fit a linear regression model to the data. Print the model's coefficient and intercept?"
      ],
      "metadata": {
        "id": "yXkXCvl2La1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#  Generate synthetic data for simple linear regression\n",
        "np.random.seed(42)  # for reproducibility\n",
        "X = 10 * np.random.rand(50, 1)         # 50 random values between 0 and 10\n",
        "y = 4 + 3 * X + np.random.randn(50, 1) # y = 4 + 3x + random noise\n",
        "\n",
        "#  Create and fit the Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print model parameters\n",
        "print(\" Simple Linear Regression Model\")\n",
        "print(f\"Coefficient (Slope): {model.coef_[0][0]:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_[0]:.4f}\")\n"
      ],
      "metadata": {
        "id": "nb7uToiLLlMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Write a Python script that fits polynomial regression models of different degrees to a synthetic dataset and compares their performance?"
      ],
      "metadata": {
        "id": "4jRmkrvfLsYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "#  Generate synthetic nonlinear dataset\n",
        "np.random.seed(42)\n",
        "X = np.linspace(0, 10, 50).reshape(-1, 1)\n",
        "y = 2 + 1.5 * X - 0.3 * X**2 + np.random.randn(50, 1) * 2  # Quadratic relation + noise\n",
        "\n",
        "#  Define polynomial degrees to test\n",
        "degrees = [1, 2, 3, 5]\n",
        "\n",
        "# Store results for comparison\n",
        "results = []\n",
        "\n",
        "#  Plot setup\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, color='blue', label='Data Points', alpha=0.6)\n",
        "\n",
        "#  Fit and evaluate models for each polynomial degree\n",
        "for degree in degrees:\n",
        "    # Transform features\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "\n",
        "    # Train model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly, y)\n",
        "\n",
        "    # Predict values\n",
        "    y_pred = model.predict(X_poly)\n",
        "\n",
        "    # Evaluate model\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "    r2 = r2_score(y, y_pred)\n",
        "    results.append((degree, mse, r2))\n",
        "\n",
        "    # Plot regression curve\n",
        "    X_new = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "    X_new_poly = poly.transform(X_new)\n",
        "    y_new = model.predict(X_new_poly)\n",
        "    plt.plot(X_new, y_new, label=f\"Degree {degree} (R¬≤={r2:.3f})\")\n",
        "\n",
        "#  Final plot formatting\n",
        "plt.title(\"Polynomial Regression Models (Different Degrees)\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#  Print performance comparison\n",
        "print(\" Polynomial Regression Model Performance Comparison:\")\n",
        "print(\"Degree\\tMSE\\t\\tR¬≤ Score\")\n",
        "for degree, mse, r2 in results:\n",
        "    print(f\"{degree}\\t{mse:.3f}\\t\\t{r2:.3f}\")\n"
      ],
      "metadata": {
        "id": "n6uBY56ELzzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.Write a Python script that fits a simple linear regression model with two features and prints the model's coefficients, intercept, and R-squared score."
      ],
      "metadata": {
        "id": "Hq-1koWCL-5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import necessary libraries\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#  Load a sample dataset (use Seaborn's 'diamonds' dataset)\n",
        "diamonds = sns.load_dataset(\"diamonds\")\n",
        "\n",
        "#  Select two features and the target variable\n",
        "# We'll predict 'price' based on 'carat' and 'depth'\n",
        "df = diamonds[['price', 'carat', 'depth']].dropna()\n",
        "\n",
        "#  Define features (X) and target (y)\n",
        "X = df[['carat', 'depth']]   # two independent variables\n",
        "y = df['price']              # dependent variable\n",
        "\n",
        "#  Split the dataset into training and testing sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#  Create and train the Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#  Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#  Evaluate model performance\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "#  Print model coefficients, intercept, and R¬≤ score\n",
        "print(\" Simple Linear Regression with Two Features\")\n",
        "print(f\"Coefficients (Slopes): {model.coef_}\")\n",
        "print(f\"Intercept: {model.intercept_:.4f}\")\n",
        "print(f\"R¬≤ Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "shhfb8b4MDyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Write a Python script that generates synthetic data, fits a linear regression model, and visualizes the regression line along with the data points"
      ],
      "metadata": {
        "id": "3Xaz5v9pMRYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#  Step 1: Generate synthetic data\n",
        "np.random.seed(42)  # for reproducibility\n",
        "X = 10 * np.random.rand(50, 1)           # 50 random X values between 0 and 10\n",
        "y = 4 + 3 * X + np.random.randn(50, 1)   # Linear relation: y = 4 + 3x + noise\n",
        "\n",
        "#  Step 2: Fit a Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Step 3: Predict values using the trained model\n",
        "X_new = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "y_pred = model.predict(X_new)\n",
        "\n",
        "#  Step 4: Print model parameters\n",
        "print(\" Linear Regression Model\")\n",
        "print(f\"Coefficient (Slope): {model.coef_[0][0]:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_[0]:.4f}\")\n",
        "\n",
        "#  Step 5: Visualize the data points and regression line\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X, y, color='blue', label='Data Points', alpha=0.7)\n",
        "plt.plot(X_new, y_pred, color='red', linewidth=2, label='Regression Line')\n",
        "plt.title(\"Linear Regression on Synthetic Data\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aFU0yHCrMWlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python script that uses the Variance Inflation Factor (VIF) to check for multicollinearity in a dataset with multiple features."
      ],
      "metadata": {
        "id": "5FanzQTcMc7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import statsmodels.api as sm\n",
        "\n",
        "#  Step 1: Generate synthetic dataset with multiple features\n",
        "X, y = make_regression(n_samples=100, n_features=4, noise=0.5, random_state=42)\n",
        "\n",
        "# Convert to DataFrame for clarity\n",
        "df = pd.DataFrame(X, columns=['Feature1', 'Feature2', 'Feature3', 'Feature4'])\n",
        "\n",
        "# Introduce some correlation to create multicollinearity\n",
        "df['Feature5'] = df['Feature1'] * 0.8 + np.random.normal(0, 0.1, 100)\n",
        "\n",
        "#  Step 2: Calculate Variance Inflation Factor (VIF)\n",
        "# Add constant column for intercept\n",
        "X_const = sm.add_constant(df)\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X_const.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_const.values, i)\n",
        "                   for i in range(X_const.shape[1])]\n",
        "\n",
        "#  Step 3: Display VIF results\n",
        "print(\" Variance Inflation Factor (VIF) for each feature:\")\n",
        "print(vif_data)\n"
      ],
      "metadata": {
        "id": "a23xKMJ8MiIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.Write a Python script that generates synthetic data for a polynomial relationship (degree 4), fits a polynomial regression model, and plots the regression curve"
      ],
      "metadata": {
        "id": "s0TyjG-VMs5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#  Step 1: Generate synthetic data (degree 4 relationship)\n",
        "np.random.seed(42)\n",
        "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "y = 2 + 0.5 * X - 1.5 * X**2 + 0.9 * X**3 - 0.3 * X**4 + np.random.randn(100, 1)\n",
        "\n",
        "#  Step 2: Create polynomial features (degree 4)\n",
        "poly = PolynomialFeatures(degree=4)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "#  Step 3: Fit a linear regression model on the transformed features\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "#  Step 4: Predict values for plotting\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "#  Step 5: Display model coefficients\n",
        "print(\" Polynomial Regression Model (Degree 4)\")\n",
        "print(\"Intercept:\", model.intercept_[0])\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "\n",
        "#  Step 6: Plot the data and regression curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X, y, color='blue', label='Data Points', alpha=0.6)\n",
        "plt.plot(X, y_pred, color='red', linewidth=2, label='Polynomial Regression Curve')\n",
        "plt.title(\"Polynomial Regression (Degree 4)\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hRrxuX-vM3yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.Write a Python script that creates a machine learning pipeline with data standardization and a multiple linear regression model, and prints the R-squared score."
      ],
      "metadata": {
        "id": "a-QGSf4nM_GQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#  Step 1: Generate synthetic data for multiple linear regression\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 3) * 10   # 3 features\n",
        "y = 5 + 2*X[:, 0] - 1.5*X[:, 1] + 3*X[:, 2] + np.random.randn(100) * 2  # linear relation with noise\n",
        "\n",
        "#  Step 2: Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#  Step 3: Create a pipeline with StandardScaler and LinearRegression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),         # Standardize features\n",
        "    ('model', LinearRegression())         # Fit linear regression model\n",
        "])\n",
        "\n",
        "#  Step 4: Train the pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "#  Step 5: Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "#  Step 6: Compute R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "#  Step 7: Display results\n",
        "print(\" Multiple Linear Regression with Standardization\")\n",
        "print(f\"R-squared Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "BLUWpQ-GNE6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.Write a Python script that performs polynomial regression (degree 3) on a synthetic dataset and plots the regression curve."
      ],
      "metadata": {
        "id": "HN8lKeYtNPtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#  Step 1: Generate synthetic data for a cubic relationship\n",
        "np.random.seed(42)\n",
        "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "y = 2 + 1.5*X - 2*X**2 + 0.5*X**3 + np.random.randn(100, 1) * 2  # cubic pattern with noise\n",
        "\n",
        "#  Step 2: Transform features to include polynomial terms (degree 3)\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "#  Step 3: Fit a Linear Regression model on the transformed features\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "#  Step 4: Predict values using the trained model\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "#  Step 5: Display model parameters\n",
        "print(\" Polynomial Regression Model (Degree 3)\")\n",
        "print(f\"Intercept: {model.intercept_[0]:.4f}\")\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "\n",
        "#  Step 6: Plot the data points and regression curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X, y, color='blue', label='Data Points', alpha=0.7)\n",
        "plt.plot(X, y_pred, color='red', linewidth=2, label='Polynomial Regression Curve')\n",
        "plt.title(\"Polynomial Regression (Degree 3)\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EZESwwfvNVHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.Write a Python script that performs multiple linear regression on a synthetic dataset with 5 features. Print the R-squared score and model coefficients."
      ],
      "metadata": {
        "id": "mMnpsIW6Nehg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        " Step 1: Generate synthetic data (5 features)\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(200, 5) * 10   # 200 samples, 5 features\n",
        "# True relationship: y = 4 + 2*X1 - 1.5*X2 + 3*X3 + 0.5*X4 - 2*X5 + noise\n",
        "y = 4 + 2*X[:,0] - 1.5*X[:,1] + 3*X[:,2] + 0.5*X[:,3] - 2*X[:,4] + np.random.randn(200) * 2\n",
        "\n",
        "#  Step 2: Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#  Step 3: Create and fit the Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#  Step 4: Predict and calculate R-squared\n",
        "y_pred = model.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "#  Step 5: Display results\n",
        "print(\" Multiple Linear Regression Results\")\n",
        "print(f\"R-squared Score: {r2:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_:.4f}\")\n",
        "print(\"Coefficients:\")\n",
        "for i, coef in enumerate(model.coef_):\n",
        "    print(f\"  Feature {i+1}: {coef:.4f}\")\n"
      ],
      "metadata": {
        "id": "HzwMUHWkNjgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python script that generates synthetic data for linear regression, fits a model, and visualizes the data points along with the regression line"
      ],
      "metadata": {
        "id": "3E7xBKnXNwUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#  Step 1: Generate synthetic data\n",
        "np.random.seed(42)  # for reproducibility\n",
        "X = 10 * np.random.rand(50, 1)               # 50 random X values between 0 and 10\n",
        "y = 4 + 3 * X + np.random.randn(50, 1)       # y = 4 + 3x + random noise\n",
        "\n",
        "#  Step 2: Fit a Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "#  Step 3: Predict values for regression line\n",
        "X_line = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "y_line = model.predict(X_line)\n",
        "\n",
        "#  Step 4: Display model parameters\n",
        "print(\" Linear Regression Model\")\n",
        "print(f\"Coefficient (Slope): {model.coef_[0][0]:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_[0]:.4f}\")\n",
        "\n",
        "# Step 5: Visualize data points and regression line\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X, y, color='blue', label='Data Points', alpha=0.7)\n",
        "plt.plot(X_line, y_line, color='red', linewidth=2, label='Regression Line')\n",
        "plt.title(\"Linear Regression on Synthetic Data\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yVzEpx5_N-YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.Create a synthetic dataset with 3 features and perform multiple linear regression. Print the model's Rsquared score and coefficients."
      ],
      "metadata": {
        "id": "Is8oXkvcOkZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#  Step 1: Generate synthetic data with 3 features\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 3) * 10  # 100 samples, 3 features\n",
        "\n",
        "# True relationship: y = 5 + 2*X1 - 1*X2 + 3*X3 + noise\n",
        "y = 5 + 2*X[:, 0] - 1*X[:, 1] + 3*X[:, 2] + np.random.randn(100) * 2\n",
        "\n",
        "#  Step 2: Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#  Step 3: Fit Multiple Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#  Step 4: Predict and evaluate R-squared\n",
        "y_pred = model.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "#  Step 5: Print model coefficients, intercept, and R¬≤\n",
        "print(\" Multiple Linear Regression Results\")\n",
        "print(f\"R-squared Score: {r2:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_:.4f}\")\n",
        "print(\"Coefficients:\")\n",
        "for i, coef in enumerate(model.coef_):\n",
        "    print(f\"  Feature {i+1}: {coef:.4f}\")\n"
      ],
      "metadata": {
        "id": "zhstmUyROng_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 21.Write a Python script that demonstrates how to serialize and deserialize machine learning models using joblib instead of pickling."
      ],
      "metadata": {
        "id": "oy_0VY_mOxto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "import joblib  # For serialization\n",
        "\n",
        "#  Step 1: Generate synthetic dataset\n",
        "np.random.seed(42)\n",
        "X = 10 * np.random.rand(50, 1)\n",
        "y = 4 + 3 * X + np.random.randn(50, 1)\n",
        "\n",
        "#  Step 2: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#  Step 3: Train a Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#  Step 4: Save the trained model using joblib\n",
        "joblib_filename = \"linear_regression_model.joblib\"\n",
        "joblib.dump(model, joblib_filename)\n",
        "print(f\" Model saved to '{joblib_filename}'\")\n",
        "\n",
        "#  Step 5: Load the model from file\n",
        "loaded_model = joblib.load(joblib_filename)\n",
        "print(\" Model loaded successfully.\")\n",
        "\n",
        "#  Step 6: Make predictions and evaluate R-squared\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared Score of loaded model: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "QWSw8rd1O4IR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python script to perform linear regression with categorical features using one-hot encoding. Use the Seaborn 'tips' dataset."
      ],
      "metadata": {
        "id": "MQZ_HDCoPFeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#  Step 1: Load the 'tips' dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "#  Step 2: Select features and target\n",
        "# We'll predict 'tip' based on 'total_bill', 'sex', and 'smoker'\n",
        "X = tips[['total_bill', 'sex', 'smoker']]\n",
        "y = tips['tip']\n",
        "\n",
        "#  Step 3: One-hot encode categorical variables\n",
        "X_encoded = pd.get_dummies(X, drop_first=True)  # drop_first=True avoids dummy variable trap\n",
        "\n",
        "#  Step 4: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#  Step 5: Fit Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#  Step 6: Make predictions and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "#  Step 7: Print model coefficients, intercept, and R¬≤ score\n",
        "print(\" Linear Regression with Categorical Features (One-Hot Encoded)\")\n",
        "print(f\"Intercept: {model.intercept_:.4f}\")\n",
        "print(\"Coefficients:\")\n",
        "for feature, coef in zip(X_encoded.columns, model.coef_):\n",
        "    print(f\"  {feature}: {coef:.4f}\")\n",
        "print(f\"R-squared Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "YfZvWwAMPJ05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 23.Compare Ridge Regression with Linear Regression on a synthetic dataset and print the coefficients and Rsquared score."
      ],
      "metadata": {
        "id": "AXW0f6CQPTfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#  Step 1: Generate synthetic dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 5) * 10  # 100 samples, 5 features\n",
        "# True relationship: y = 3 + 2*X1 - X2 + 1.5*X3 + noise\n",
        "y = 3 + 2*X[:,0] - 1*X[:,1] + 1.5*X[:,2] + np.random.randn(100) * 2\n",
        "\n",
        "#  Step 2: Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#  Step 3: Train standard Linear Regression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train, y_train)\n",
        "y_pred_lin = lin_reg.predict(X_test)\n",
        "r2_lin = r2_score(y_test, y_pred_lin)\n",
        "\n",
        "#  Step 4: Train Ridge Regression (alpha=1.0)\n",
        "ridge_reg = Ridge(alpha=1.0)\n",
        "ridge_reg.fit(X_train, y_train)\n",
        "y_pred_ridge = ridge_reg.predict(X_test)\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "\n",
        "#  Step 5: Print results\n",
        "print(\" Linear Regression Results\")\n",
        "print(f\"R-squared Score: {r2_lin:.4f}\")\n",
        "print(f\"Coefficients: {lin_reg.coef_}\")\n",
        "print(f\"Intercept: {lin_reg.intercept_:.4f}\\n\")\n",
        "\n",
        "print(\" Ridge Regression Results\")\n",
        "print(f\"R-squared Score: {r2_ridge:.4f}\")\n",
        "print(f\"Coefficients: {ridge_reg.coef_}\")\n",
        "print(f\"Intercept: {ridge_reg.intercept_:.4f}\")\n"
      ],
      "metadata": {
        "id": "kT53JZnEPW2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.Write a Python script that uses cross-validation to evaluate a Linear Regression model on a synthetic dataset."
      ],
      "metadata": {
        "id": "yU3N8I2DPgrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "#  Step 1: Generate synthetic dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 3) * 10   # 100 samples, 3 features\n",
        "y = 5 + 2*X[:,0] - 1*X[:,1] + 1.5*X[:,2] + np.random.randn(100) * 2  # Linear relationship with noise\n",
        "\n",
        "#  Step 2: Create Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Step 3: Set up 5-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "#  Step 4: Perform cross-validation using R-squared as scoring metric\n",
        "cv_scores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n",
        "\n",
        "#  Step 5: Print results\n",
        "print(\" Cross-Validation Results\")\n",
        "print(f\"R-squared scores for each fold: {cv_scores}\")\n",
        "print(f\"Mean R-squared score: {np.mean(cv_scores):.4f}\")\n",
        "print(f\"Standard deviation of R-squared: {np.std(cv_scores):.4f}\")\n"
      ],
      "metadata": {
        "id": "VJZ_66mMPl0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.Write a Python script that compares polynomial regression models of different degrees and prints the Rsquared score for each."
      ],
      "metadata": {
        "id": "1sOuUbv1PujQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#  Step 1: Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "y = 2 + 0.5*X - 1.5*X**2 + 0.9*X**3 - 0.3*X**4 + np.random.randn(100, 1)  # true degree 4\n",
        "\n",
        "#  Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#  Step 3: Compare polynomial degrees\n",
        "degrees = [1, 2, 3, 4, 5]  # degrees to compare\n",
        "\n",
        "for degree in degrees:\n",
        "    # Transform features to polynomial features\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_train_poly = poly.fit_transform(X_train)\n",
        "    X_test_poly = poly.transform(X_test)\n",
        "\n",
        "    # Fit Linear Regression on polynomial features\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_poly, y_train)\n",
        "\n",
        "    # Predict and calculate R-squared\n",
        "    y_pred = model.predict(X_test_poly)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Degree {\n"
      ],
      "metadata": {
        "id": "xUAvN-qtPzGI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}